[ { "title": "Soluci√≥n al escenario Break my SSH de Dockerlabs", "url": "/posts/dockerlabs-break-my-ssh/", "categories": "ciberseguridad", "tags": "ciberseguridad, dockerlabs", "date": "2026-01-30 18:25:00 +0100", "content": "Seguimos con otro lab, donde en esta ocasi√≥n tendremos que vulnerar el servicio SSH del servidor v√≠ctima para poder completarlo. Como siempre, empezamos viendo cual es la IP y haciendo un escaneo de puertos. La IP del contenedor es la 172.17.0.2. Escaneo de puertos Realizamos el escaneo de puertos: nmap -sS 172.17.0.2 Resultado: Obtenemos que tenemos acceso por el puerto 22 (SSH). Hacemos un escaneo m√°s extenso sobre dicho puerto para obtener m√°s informaci√≥n al respecto. nmap -sV -p 22 -vvv 172.17.0.2 Resultado: Nos devuelve la versi√≥n OpenSSH 7.7, la cual tiene una vulnerabilidad CVE-2018-15473, la cual nos permite hacer una enumeraci√≥n de usuarios con los que poder conectarnos. Vamos a probar a hacer un ataque de fuerza bruta usando el listado de top usernames que da SecList. hydra -l /usr/share/seclist/SecLists-master/Usernames/top-usernames-shortlist.txt -P /usr/share/wordlists/rockyou.txt ssh://172.17.0.2 Resultado: Ah√≠ sale que la contrase√±a del usuario root es estrella. En este punto tuve que parar el proceso antes de que terminase porque parec√≠a que mi ordenador iba a despegar de un momento a otro. Probamos a conectarnos directamente con el usuario root, ponemos la contrase√±a que hemos obtenido antes y vemos que ya tenemos acceso al servidor. Con esto ya habremos terminado el lab. Espero que os haya gustado y os haya servido de ayuda. ¬°Hasta la pr√≥xima!" }, { "title": "Soluci√≥n al escenario FirstHacking de Dockerlabs", "url": "/posts/dockerlabs-first-hacking/", "categories": "ciberseguridad", "tags": "ciberseguridad, dockerlabs", "date": "2026-01-27 18:25:00 +0100", "content": "Siguiendo con la serie de posts sobre ciberseguridad, vamos a por otro lab muy sencillo pero que nos ayudar√° con el tema de saber buscar vulnerabilidad por Google y explotarlas con MetaSploit. La IP del contenedor es la 172.17.0.2, la cual te sale seg√∫n se despliega el lab. Escaneo de puertos Realizamos el escaneo de puertos: nmap -sS 172.17.0.2 Resultado: Obtenemos que hay un puerto abierto, el 21 (FTP). Hacemos un escaneo del puerto para obtener m√°s informaci√≥n al respecto. nmap -sV -p 21 -vvv 172.17.0.2 Resultado: Si hacemos una b√∫squeda r√°pida en google sobre ‚Äúexploit vsftpd 2.3.4‚Äù, nos sale que est√° relacionada con el CVE-2011-2523. En rapid7 cuentan con un m√≥dulo para poder explotar dicha vulnerabilidad, https://www.rapid7.com/db/modules/exploit/unix/ftp/vsftpd_234_backdoor/ Abrimos msfconsole, cargamos el m√≥dulo, seteamos el host destino y ejecutamos. Con esto ya tenemos acceso root al servidor. Espero que os haya gustado y os haya servido de ayuda. ¬°Hasta la pr√≥xima!" }, { "title": "Soluci√≥n al escenario Trust de Dockerlabs", "url": "/posts/dockerlabs-trust/", "categories": "ciberseguridad", "tags": "ciberseguridad, dockerlabs", "date": "2026-01-23 18:25:00 +0100", "content": "Aparte de hacer las cosas propias de un administrador de sistemas, de vez en cuando me da por curiosear cosas de ciberseguridad, aunque realmente no sea mi fuerte. Como llevo ya un tiempo que me ha dado por documentar las cosas que voy haciendo, aprovecho y hago un post al respecto. Una buena forma de empezar en el mundillo con labs que son muy interesantes son los que te proporcionan en Dockerlabs, que tienen varios labs con distintos niveles, por lo que os recomiendo mucho que le ech√©is un vistazo a su web, os hag√°is una cuenta y hag√°is los labs que m√°s os llamen la atenci√≥n. En este primer post, empezaremos por un lab de dificultad muy f√°cil, Trust. Comentar que en estos posts, s√≥lo pondr√© los pasos que he hecho yo para llegar a la soluci√≥n, pasos como el despliegue del contenedor me lo saltar√©, ya que en la propia web viene indicado de forma bastante clara. Con todo esto dicho, vamos al l√≠o. La IP del contenedor es la 172.18.0.2, la cual te sale seg√∫n se despliega el lab. Escaneo de puertos Realizamos un escaneo para ver que puertos est√°n abiertos: nmap -sS 172.18.0.2 Resultado: Obtenemos que hay dos puertos abiertos, el 22 (SSH) y 80 (HTTP). Hacemos un escaneo de esos puertos para obtener m√°s informaci√≥n al respecto. nmap -sV -p 22,80 -vvv 172.18.0.2 Resultado: Nos conectamos por firefox a la URL y nos sale la p√°gina por defecto de Apache2: Para poder continuar vamos a hacer un escaneo con el comando gobuster para obtener un listado de ficheros o directorios a los que se pueda acceder desde el servidor web. gobuster dir -u http://172.18.0.2 -w /usr/share/wordlists/dirbuster/directory-list-2.3-medium.txt -x php,html,txt Resultado: Vemos que hay disponibles, index.html, que es la p√°gina de inicio, secret.php y /server-status, pero este √∫ltimo nos da un 403. Si nos conectamos a secret.php nos aparece el siguiente mensaje: Vamos a revisar el c√≥digo fuente de la web por si encontr√°semos algo interesante al respecto, aunque en este caso no es as√≠. Vamos a probar ahora con SSH y con el usuario mario que nos ha salido al visitar secret.php. Para ello, vamos a usar la herramienta hydrapara sacar la contrase√±a con un ataque de fuerza bruta (previamente hay que descomprimir el fichero /usr/share/wordlists/rockyou.txt.gz). hydra -l mario -P /usr/share/wordlists/rockyou.txt ssh://172.18.0.2 Resultado: Probamos a acceder con la contrase√±a que nos ha salido: Vamos a comprobar si el usuario puede ejecutar alg√∫n binario como sudo con el siguiente comando. sudo -l Y vemos que puede ejecutar vim Miramos en la URL https://gtfobins.github.io/ c√≥mo podemos explotar ese acceso sudo a vim. La URL final donde podemos encontrar esto es https://gtfobins.github.io/gtfobins/vim/#sudo Ejecutamos el primer comando y comprobamos que usuario somos despu√©s de eso: Con esto hemos conseguido escalar privilegios y por lo tanto, terminar el lab. Espero que os haya gustado y os haya servido de ayuda. ¬°Hasta la pr√≥xima!" }, { "title": "Configuraci√≥n avanzada en KVM", "url": "/posts/kvm-configuracion-avanzada/", "categories": "linux, virtualizacion", "tags": "linux, kvm, virtualizacion", "date": "2026-01-21 18:25:00 +0100", "content": "Con este post terminaremos (de momento) la miniserie relacionada con KVM, donde veremos comandos m√°s avanzados, que nos permitir√°n gestionar KVM al siguiente nivel. Ver o editar XML de una VM virsh dumpxml &lt;nombre&gt; virsh edit &lt;nombre&gt; Monitorizaci√≥n Ver uso de CPU, RAM y red de todas las VMs virsh domstats --state --cpu-total --balloon --interface Ver interfaces de red activas ip link show | grep virbr Ver interfaces de una VM virsh domiflist &lt;nombre&gt; De momento he planteado estos 4 posts para manejar KVM, pero seg√∫n vaya trabajando m√°s con ello, ir√© actualizando o agregando m√°s posts al respecto. Espero que os haya gustado y os haya servido de ayuda. ¬°Hasta la pr√≥xima!" }, { "title": "Gesti√≥n de redes y almacenamiento en KVM", "url": "/posts/kvm-redes-almacenamiento/", "categories": "linux, virtualizacion", "tags": "linux, kvm, virtualizacion", "date": "2026-01-17 18:25:00 +0100", "content": "Para este post podr√≠a haber hecho dos diferentes, pero he pensado que como iban a ser muy cortos, mejor ser√≠a juntarlo en uno solo. Empecemos con la parte de almacenamiento y los comandos para gestionar los discos y los pools. Gestionar almacenamiento Listar pools virsh pool-list --all Informaci√≥n y contenido virsh pool-info &lt;pool&gt; virsh vol-list &lt;pool&gt; Creaci√≥n de un nuevo volumen/disco virsh vol-create-as default vm1.qcow2 20G --format qcow2 Y para terminar, los comandos relacionados con las redes. Gestionar redes Listar redes virsh net-list --all La carpeta donde se dejan los ficheros de configuraci√≥n en Ubuntu por defecto es /etc/libvirt/qemu Informaci√≥n de red virsh net-info &lt;nombre&gt; # Con este comando nos mostrar√° la info en un XML virsh net-dumpxml &lt;nombre&gt; Modificar una red virsh net-edit &lt;nombre&gt; Nos abrir√° un editor de texto y podremos configurar el XML Crear una red Creamos un fichero XML con el siguiente formato, el fichero para este ejemplo lo voy a llamar red_nueva.xml &lt;network&gt; &lt;name&gt;red_prueba&lt;/name&gt; &lt;bridge name='virbr2'/&gt; &lt;ip address='192.168.100.1' netmask='255.255.255.0'&gt; &lt;dhcp&gt; &lt;range start='192.168.100.2' end='192.168.100.254'/&gt; &lt;/dhcp&gt; &lt;/ip&gt; &lt;/network&gt; Y aplicamos el fichero, donde el nombre es el que hemos definido en la parte de &lt;name&gt;¬†del fichero XML: virsh net-define red_nueva.xml virsh net-start red_prueba virsh net-autostart red_prueba Destruir y eliminar red virsh net-destroy &lt;nombre&gt; virsh net-undefine &lt;nombre&gt; Aunque estos posts m√°s cortitos y que son un listado de comandos no tengan tanta chicha como otros m√°s elaborados, vienen muy bien para tener una chuleta con todo esto. Espero que os haya gustado y os haya servido de ayuda. ¬°Hasta la pr√≥xima!" }, { "title": "Conceptos y comandos b√°sicos de KVM", "url": "/posts/kvm-conceptos-basicas/", "categories": "linux, virtualizacion", "tags": "linux, kvm, virtualizacion", "date": "2026-01-15 18:25:00 +0100", "content": "Siguiendo con la t√©matica del post anterior, hoy vamos a ver conceptos y comandos b√°sicos que se pueden utilizar en cualquier entorno que utilice KVM y poder hacer una gesti√≥n minima de los recursos que nos ofrece este virtualizador. Empecemos con la creaci√≥n de una VM. Creaci√≥n de una VM con virt-install virt-install \\ --name=vmtest \\ --ram=4096 \\ --vcpus=2 \\ --disk path=/var/lib/libvirt/images/vmtest/vmtest.qcow2,size=50 \\ --cdrom /var/lib/libvirt/images/ubuntu-24.04.3-live-server-amd64.iso \\ --network network=default \\ --graphics vnc Donde: --name¬†es el nombre de la VM --ram¬†la RAM en MB de la VM --vcpus las CPUs --disk path= la ruta donde crear√° el disco de la VM en formato .qcow2, size=¬†es el tama√±o del disco en GB --cdrom ruta y fichero con la ISO para hacer la instalaci√≥n de la VM --network la red donde estar√° la VM --graphics si la VM debe tener gr√°ficos, una pantalla antes de poder conectarnos por SSH. Existe la opci√≥n de vnc¬†y none. El resto del post ser√°n comandos que nos permitir√° hacer lo que se indica en el t√≠tulo del mismo. Gesti√≥n de m√°quinas virtuales Listar VMs # VMs en ejecuci√≥n virsh list # Todas las VMs, incluidas las apagadas virsh list --all Informaci√≥n de una VM virsh dominfo &lt;nombre&gt; Iniciar/apagar/reiniciar una VM virsh start &lt;nombre&gt; virsh shutdown &lt;nombre&gt; virsh reboot &lt;nombre&gt; # Apagado forzado virsh destroy &lt;nombre&gt; Autostart de una VM # Activar inicio autom√°tico virsh autostart &lt;nombre&gt; # Desactivar inicio autom√°tico virsh autostart --disable &lt;nombre&gt; Eliminar una VM virsh undefine &lt;nombre&gt; Tener en cuenta que no elimina el disco, para eliminar el disco: rm /var/lib/libvirt/images/&lt;nombre&gt;.qcow2 Snapshots Crear snapshot virsh snapshot-create-as &lt;nombre&gt; snapshot1 \"Snapshot inicial\" Ver snapshots virsh snapshot-list &lt;nombre&gt; Revertir snapshot virsh snapshot-revert &lt;nombre&gt; snapshot1 Espero que os haya gustado y os haya servido de ayuda. ¬°Hasta la pr√≥xima!" }, { "title": "Instalar KVM en Ubuntu", "url": "/posts/kvm-instalar-ubuntu/", "categories": "linux, virtualizacion", "tags": "linux, kvm, virtualizacion", "date": "2026-01-13 18:25:00 +0100", "content": "Hace poco tuve que desplegar un servidor con bastante recursos al que no pod√≠amos instalarle proxmox para sacarle todo su potencial y desplegar varias VMs en √©l, as√≠ que decidimos utilizar KVM y aprovech√© para documentar varios apartados de c√≥mo utilizarlo. En este post empezaremos por el principio, instalando KVM y las librer√≠as necesarias en un servidor Ubuntu. Pasos previos Antes de ponernos a instalar, tenemos que comprobar ciertas cosas. Comprobar si Ubuntu soporta la virtualizaci√≥n Ejecutamos el siguiente comando egrep -c '(vmx|svm)' /proc/cpuinfo Si el comando devuelve un valor distinto a 0, nuestro procesador no tiene la capacidad de de virtualizar y no podremos seguir este post. Comprobar si Ubuntu puede usar la aceleraci√≥n KVM Ejecutamos el siguiente comando sudo kvm-ok Si nos devuelve KVM acceleration can be used, nuestro Ubuntu podr√° usar KVM. Con esto ya podremos pasar a la instalaci√≥n. Instalar los paquetes de KVM sudo apt install qemu-kvm libvirt-daemon-system libvirt-clients bridge-utils virtinst -y Agregar usuarios al grupo de KVM Para no tener que estar con un usuario con permisos de sudo, agregaremos a un usuario al grupo libvirt para que tenga los permisos necesarios para ejecutar los comandos. sudo adduser $USER libvirt Y al grupo kvm sudo adduser $USER kvm Verificar instalaci√≥n Para comprobar que todo funciona correctamente, ejecutamos el comando virsh list --all¬†y no devuelve ning√∫n error, la instalaci√≥n se habr√° hecho correctamente. Con esto ya tendremos KVM instalado, en los siguientes posts veremos m√°s comandos y utilidades para desplegar m√°quinas, configurar redes, almacenamiento y configuraci√≥n m√°s avanzada. Espero que os haya gustado y os haya servido de ayuda. ¬°Hasta la pr√≥xima!" }, { "title": "Configurar el log Slow Query en MySQL o MariaDB", "url": "/posts/mysql-configurar-slow-query/", "categories": "linux, mysql", "tags": "linux, mysql, mariadb", "date": "2026-01-11 17:25:00 +0100", "content": "En este post veremos c√≥mo activar y configurar el log de Slow Query de MySQL/MariaDB, una herramienta que nos dir√° que consultas hacen que nuestra BDD lo pase mal. Para poder activarlo, tenemos dos opciones: Configurarlo de forma temporal, cuando se reinicie el servicio volver√° a su estado por defecto. Configurarlo de forma permanente. Configuraci√≥n temporal Para realizar la configuraci√≥n de manera temporal, tenemos que acceder a la BDD y ejecutar las siguientes queries, las cuales indicar√© que hace cada una. Activa (si ponemos 0 se desactiva) el log de slow query. SET GLOBAL slow_query_log = 1; Definimos el tiempo en segundos para considerar una consulta como ‚Äúlenta‚Äù. SET GLOBAL long_query_time = 10; Activamos para que registre consultas que no usan √≠ndices, incluso si son r√°pidas. Si ponemos 0, s√≥lo registrar√° las queries que excedan el tiempo definido en long_query_time. Este par√°metro viene bien si queremos hacer auditorias, pero habr√≠a que tener cuidado con el tama√±o del log. SET GLOBAL log_queries_not_using_indexes = 0; Configuraci√≥n permanente En este paso tenemos que tener localizado el fichero my.cnf, que normalmente se encuentra en /etc/mysql. Editamos el fichero y a√±adimos la siguietne configuraci√≥n: [mysqld] ¬†¬† slow_query_log = 1¬†¬† long_query_time = 1¬†¬† slow_query_log_file = /var/log/mysql/slow.log¬†¬† log_error = /var/log/mysql/mariadb-error.log¬†¬† general_log = 0 general_log captura todo los registros en el servidor. Aqu√≠ lo pongo en 0 porque en producci√≥n afectar√≠a mucho a nivel de I/O, tama√±o del log y al rendimiento en general. Una vez terminado, reiniciamos el servicio de MySQL/MariaDB para aplicar los cambios. Comprobar configuraci√≥n Para comprobar que la configuraci√≥n que hemos realizado se ha aplicado correctamente accedemos a la BDD y ejecutamos las siguientes queries. MariaDB [(none)]&gt; SHOW VARIABLES LIKE 'slow_query_log'; +----------------+-------+ | Variable_name | Value | +----------------+-------+ | slow_query_log | ON | +----------------+-------+ 1 row in set (0.001 sec) MariaDB [(none)]&gt; SHOW VARIABLES LIKE 'slow_query_log_file'; +---------------------+-------------------------+ | Variable_name | Value | +---------------------+-------------------------+ | slow_query_log_file | /var/lib/mysql/slow.log | +---------------------+-------------------------+ 1 row in set (0.000 sec) MariaDB [(none)]&gt; SHOW VARIABLES LIKE 'long_query_time'; +-----------------+----------+ | Variable_name | Value | +-----------------+----------+ | long_query_time | 10.000000 | +-----------------+----------+ 1 row in set (0.001 sec) Y con esto ya tendremos lista la configuraci√≥n para ver qu√© queries son las que est√°n afectando a nuestra BDD. Espero que os haya gustado y os haya servido de ayuda. ¬°Hasta la pr√≥xima!" }, { "title": "Hacer backup de una BDD en MySQL", "url": "/posts/mysql-hacer-backup/", "categories": "linux, mysql", "tags": "linux, mysql, mariadb", "date": "2026-01-09 17:25:00 +0100", "content": "Por si no os hab√©is dado cuenta, he tenido que empezar a trabajar m√°s con MySQL, haciendo que pueda escribir posts al respecto. El de hoy va sobre un comando muy simple para hacer backups de forma f√°cil y c√≥moda. Con el siguiente comando podemos hacer un backup y que lo guarde en el mismo directorio en el que estamos con la fecha que se ha realizado. MYSQL_DATABASE=bbd_test mysqldump -u root -p$MYSQL_PASSWORD --single-transaction --quick $MYSQL_DATABASE | gzip &gt; bk.$MYSQL_DATABASE.`date +%d%m%Y`.sql.gz He definido la variable MYSQL_DATABASE para no tener que poner el nombre de la BDD varias veces. En caso de que no tener definida la variable MYSQL_PASSWORD, tendremos que quitar la primera variable en el comando para que nos pida la contrase√±a y tener que ponerla a mano. Esto podremos ponerlo en el cron de nuestro servidor y que as√≠ que vayan haciendo los backups de forma autom√°tica. Espero que os haya gustado y os haya servido de ayuda. ¬°Hasta la pr√≥xima!" }, { "title": "Instalar y configurar un cl√∫ster b√°sico de Kubernetes con Kind", "url": "/posts/instalar-configurar-kubernetes-basico-kind/", "categories": "kubernetes", "tags": "kubernetes", "date": "2026-01-07 17:25:00 +0100", "content": "Empezar con Kubernetes siempre es una odisea, m√°s si no tienes la m√≠nima idea de por d√≥nde dar el primer paso, lo cual a todos nos ha pasado alguna vez. Por suerte, para eso est√° este post, el cual nos dar√° una buena base para poder empezar y poder trastear con Kubernetes. Para ello, usaremos Kind, la cual es una herramienta para levantar cl√∫steres locales de Kubernetes mediante contenedores de Docker. Antes que nada tendremos que tener instalado Docker (en el blog hay posts indicando c√≥mo instalarlo) en el equipo donde vayamos a utilizar Kind. Instalaci√≥n Para instalarlo en un Linux tendremos que ejecutar el siguiente comando, dando por hecho que se va a instalar en un equipo x86_64: curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.30.0/kind-linux-amd64 chmod +x ./kind sudo mv ./kind /usr/local/bin/kind Para instarlalo en un Windows, desde Powershell ejecutamos el comando: curl.exe -Lo kind-windows-amd64.exe https://kind.sigs.k8s.io/dl/v0.30.0/kind-windows-amd64 Move-Item .\\kind-windows-amd64.exe c:\\some-dir-in-your-PATH\\kind.exe Si ejecutamos kind, nos aparecer√° lo siguiente: $ kind kind creates and manages local Kubernetes clusters using Docker container 'nodes' Usage: kind [command] Available Commands: build Build one of [node-image] completion Output shell completion code for the specified shell (bash, zsh or fish) create Creates one of [cluster] delete Deletes one of [cluster] export Exports one of [kubeconfig, logs] get Gets one of [clusters, nodes, kubeconfig] help Help about any command load Loads images into nodes version Prints the kind CLI version Flags: -h, --help help for kind -q, --quiet silence all stderr output -v, --verbosity int32 info log verbosity, higher value produces more output --version version for kind Use \"kind [command] --help\" for more information about a command. Con esto ya podremos usarlo. Configuraci√≥n b√°sica A continuaci√≥n adjunto un fichero yaml b√°sico para desplegar un cl√∫ster con un control-plane y dos workers. kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 name: cluster-local networking: apiServerAddress: \"127.0.0.1\" apiServerPort: 6443 nodes: - role: control-plane image: kindest/node:v1.34.0@sha256:7416a61b42b1662ca6ca89f02028ac133a309a2a30ba309614e8ec94d976dc5a extraPortMappings: - containerPort: 30000 hostPort: 30000 protocol: TCP - role: worker image: kindest/node:v1.34.0@sha256:7416a61b42b1662ca6ca89f02028ac133a309a2a30ba309614e8ec94d976dc5a extraPortMappings: - containerPort: 30000 hostPort: 30001 protocol: TCP - role: worker image: kindest/node:v1.34.0@sha256:7416a61b42b1662ca6ca89f02028ac133a309a2a30ba309614e8ec94d976dc5a extraPortMappings: - containerPort: 30000 hostPort: 30002 protocol: TCP Donde name¬†es el nombre del cluster. Para este ejemplo estamos usando la versi√≥n 1.34.0 (Podemos ver las versiones que tienen disponibles en https://github.com/kubernetes-sigs/kind/releases). Desplegar cl√∫ster Para desplegarlo tendremos que ejecutar el comando y nos devolver√° lo siguiente: ‚îî‚îÄ$ kind create cluster --config=kind-config.yml Creating cluster \"cluster-local\" ... ‚úì Ensuring node image (kindest/node:v1.34.0) üñº ‚úì Preparing nodes üì¶ üì¶ üì¶ ‚úì Writing configuration üìú ‚úì Starting control-plane üïπÔ∏è ‚úì Installing CNI üîå ‚úì Installing StorageClass üíæ ‚úì Joining worker nodes üöú Set kubectl context to \"kind-cluster-local\" You can now use your cluster with: kubectl cluster-info --context kind-cluster-local Have a nice day! üëã Si ejecutamos cualquier comando de kubectl, nos devolver√° los datos del cl√∫ster que acabamos que desplegar. ‚îî‚îÄ$ kubectl get nodes NAME STATUS ROLES AGE VERSION cluster-local-control-plane Ready control-plane 48s v1.34.0 cluster-local-worker Ready &lt;none&gt; 36s v1.34.0 cluster-local-worker2 Ready &lt;none&gt; 36s v1.34.0 Eliminar cl√∫ster Si queremos eliminarlo, tendremos que ejecutar el siguiente comando, indicando en name¬†el nombre del cl√∫ster que hemos definido en el fichero: kind delete cluster --name=cluster-local Con todo esto, tendremos un cluster de Kubernetes totalmente funcional listo para que hagamos cosas en √©l, que iremos viendo en futuros posts. Espero que os haya gustado y os haya servido de ayuda. ¬°Hasta la pr√≥xima!" }, { "title": "Instalar/renovar certificado en FreeIPA para HTTPS y LDAP", "url": "/posts/instalar-certificado-freeipa/", "categories": "linux, freeipa", "tags": "linux, freeipa", "date": "2026-01-04 17:25:00 +0100", "content": "En este post veremos c√≥mo podemos instalar un certificado externo en nuestro servidor FreeIPA para no utilizar el autofirmado que genera por defecto. Vamos a instalarlo tanto para que se muestre en la web como para el servicio de LDAP y para ello, tenemos que realizar los siguientes pasos. Disclamer: Estoy dando por hecho que ya tenemos un certificado con una CA v√°lida para poder hacer todo esto. El fichero con la clave p√∫blica tambi√©n tiene que tener la CA en ese mismo fichero, no vale tenerlo en un fichero aparte. Instalaci√≥n del certificado Para poder instalar el certificado tenemos que ejecutar el siguiente comando, indicando que la instalaci√≥n ser√° para HTTP y el servicio de directorio activo. ipa-server-certinstall --http --dirsrv &lt;certificado&gt; &lt;key&gt; Cuando le demos Enter nos pedir√° la contrase√±a del Directory Manager y la clave de la private key en caso de que esta tuviese alguna. Aplicamos la configuraci√≥n Una vez hecho esto, reiniciamos los servicios. ipactl restart systemctl restart httpd.service systemctl restart dirsrv@&lt;nombre_dominio&gt;.service Donde &lt;nombre_dominio&gt; se puede sacar del directorio /etc/dirsrv/slapd-* teniendo el formato SAMURANTECH-COM. Comprobamos los servicios Con los siguientes comandos veremos si los cambios se ha realizado correctamente. certutil -L -d /etc/httpd/alias certutil -L -d /etc/dirsrv/slapd-&lt;nombre_dominio&gt; Nos tendr√° que aparecer que el certificado que usa es el que hemos instalado al principio. Con esto ya tendremos todo instalado y configurado para que nuestro servidor de FreeIPA utilice un certificado externo, haciendo que cuando nos conectemos por HTTPS no aparezca que usa uno autofirmado. Espero que os haya gustado y os haya servido de ayuda. ¬°Hasta la pr√≥xima!" }, { "title": "Desactivar IPv6 en Cloudflare", "url": "/posts/desactivar-ipv6-cloudflare/", "categories": "cloudflare", "tags": "cloudflare", "date": "2026-01-03 18:25:00 +0100", "content": "Aunque IPv6 sea el futuro, conozco a m√°s de uno que se ha tenido que pelear con √©l y ha tenido m√°s problemas que otra cosa. Por eso en este post, vamos a ver c√≥mo desactivar IPv6 para Cloudflare, ya que no es dar a un click y listo. Disclaimer: A d√≠a 13 de noviembre de 2025, d√≠a en el que me toc√≥ hacer esto, este m√©todo era v√°lido. Para poder desactivar la compatibilidad con IPv6 hay que hacerlo v√≠a API, ya que por interfaz web est√° desactivada. Se encuentra en la parte de Network/Red: Para poder hacerlo, previamente hay que crear un API token que tenga permisos de editar el par√°metro Zone.Zone Settings. Para ver c√≥mo crear uno, en el post anterior est√° indicado c√≥mo hacerlo. Una vez creado, tendremos que ejecutar el siguiente curl: curl -XPATCH https://api.cloudflare.com/client/v4/zones/&lt;id_zone&gt;/settings/ipv6 -H 'Content-Type: application/json' --header \"Authorization: Bearer ...\" -d '{\"value\":\"off\"}' La &lt;id_zone&gt; se puede obtener del apartado API del pantallazo anterior: Con el curl ejecutado, ya aparecer√° desactivado. Para poder volver a activarlo, ser√≠a cambiar el valor de off a on. curl -XPATCH https://api.cloudflare.com/client/v4/zones/&lt;id_zone&gt;/settings/ipv6 -H 'Content-Type: application/json' --header \"Authorization: Bearer ...\" -d '{\"value\":\"on\"}' Con esto ya sabremos c√≥mo desactivar IPv6 en Cloudflare, ahorr√°ndonos varios dolores de cabeza. Espero que os haya gustado y os haya servido de ayuda. ¬°Hasta la pr√≥xima!" }, { "title": "Crear API Token en Cloudflare", "url": "/posts/crear-apitoken-cloudflare/", "categories": "cloudflare", "tags": "cloudflare", "date": "2026-01-02 18:25:00 +0100", "content": "Si trabaj√°is con Cloudflare, este post os servir√° de mucho, ya que vamos a empezar creando un token que nos permita automatizar despliegues y configuraciones que nos ahorrar√°n much√≠simo trabajo. Para empezar, tendremos que ir a nuestra cuenta y en la parte superior derecha, donde sale el monigote, le damos a Profile o Perfil seg√∫n el idioma en el que lo tengas. En la secci√≥n de Profile vamos a API Tokens y le damos a Create Token. Cloudflare te recomienda usar Account API Tokens en caso de no tener una cuenta de servicio para estas cosas, pero al caso el proceso es el mismo. En la siguiente pantalla nos saldr√°n varias plantillas con permisos para ciertos escenarios, pero yo usar√© en este caso el Create Custom Token. Aqu√≠ tendremos que poner el nombre del API Token, los permisos que queremos que tenga (si es sobre la cuenta o una zona DNS), si queremos que tenga permisos sobre todas las zonas o sobre una en concreto y un filtrado por IP para que s√≥lo se pueda utilizar desde donde nosotros indiquemos. En este caso, le hemos dado permisos para que pueda editar la configuraci√≥n de la zona DNS y que s√≥lo sea para un dominio en espec√≠fico. Para continuar nos saldr√° un resumen de lo que hemos hecho y podremos finalizar el proceso una vez le hayamos dado a Create Token. Con esto ya sabremos c√≥mo crear un API Token en Cloudflare, el cual usaremos en futuros posts. Espero que os haya gustado y os haya servido de ayuda. ¬°Hasta la pr√≥xima!" }, { "title": "Cambiar contrase√±a a un usuario en MySQL", "url": "/posts/mysql-cambiar-password/", "categories": "linux, mysql", "tags": "linux, mysql, mariadb", "date": "2025-12-31 18:25:00 +0100", "content": "Otro post cortito enfocado a MySQL, esta vez para cambiar la contrase√±a de cualquier usuario. Para poder hacerlo, tenemos que ejecutar el siguiente comando desde la propia consola de MySQL. MariaDB [(none)]&gt; ALTER USER 'username'@'host' IDENTIFIED BY 'password'; FLUSH PRIVILEGES; Esto vale para cualquier usuario, ya sea uno de sistema, uno n√≥minal o el propio usuario root. Si no sabemos qu√© usuarios hay en MySQL, en el post anterior habl√© sobre c√≥mo poder consultarlos. Tengo que indicar que si ejecutamos el comando as√≠, estaremos usando el plugin de autenticaci√≥n por defecto del sistema, existen otros pero no voy a profundizar sobre ello aqu√≠. De momento hay que saber que si ponemos alguna opci√≥n distinta, podremos hacer que aplicaciones legacy o clientes existentes dejen de funcionar m√°s all√° de haber cambiado la contrase√±a, y esto es un verdadero dolor de cabeza cuando no ca√©is en que falla por esto. Con esto ya sabremos c√≥mo cambiar la contrase√±a de los usuarios y damos pie a otro post donde profundizar√© en los tipos de plugins de autenticaci√≥n en MySQL. Espero que os haya gustado y os haya servido de ayuda. ¬°Hasta la pr√≥xima!" }, { "title": "Ver listado de usuarios/permisos en MySQL", "url": "/posts/mysql-ver-usuarios-permisos/", "categories": "linux, mysql", "tags": "linux, mysql, mariadb", "date": "2025-12-30 18:25:00 +0100", "content": "Este post ser√° algo breve aunque muy √∫til si os pasa como a m√≠ que est√° m√°s acostumbrado a trabajar con PostgreSQL y ten√©is que cambiar a MySQL, parec√©is un elefante en una cacharrer√≠a. Para poder ver los usuarios creados en un servidor MySQL, tendremos que ejecutar el siguiente comando desde la propia consola de MySQL para que muestre el usuario y los hosts desde los que tiene permitido acceder. Con este comando no tendremos que elegir la BDD de mysql previamente porque ya se lo estamos indicando en el FROM. MariaDB [(none)]&gt; SELECT user,host FROM mysql.user; El host %¬†significa que puede acceder desde que cualquier sitio. Permisos Para poder ver los permisos asociados tendremos que ejecutar el siguiente comando desde la propia consola de MySQL. MariaDB [(none)]&gt; show grants for 'usuario'@'%'; El formato del usuario tiene que ser 'usuario'@'host', ya que en MySQL se puede distinguir los permisos seg√∫n el host adem√°s de por BDD. El output del comando ser√≠a el siguiente. La primera l√≠nea, la de GRANT USAGE... es para indicar que el usuario existe para el host indicado y con la contrase√±a hasheada. La segunda l√≠nea, la de GRANT ALL... indica sobre qu√© BDD tiene permisos y qu√© permisos tiene, en este ejemplo tiene todos los que se puede tener (SELECT, UPDATE, DELETE, INSERT, CREATE, DROP‚Ä¶). Con esto ya sabremos qu√© usuarios tenemos en nuestro MySQL, adem√°s sobre qu√© puede tocar y qu√© no. Espero que os haya gustado y os haya servido de ayuda. ¬°Hasta la pr√≥xima!" }, { "title": "Comprobar conectivad contra un puerto usando curl", "url": "/posts/curl-conectividad-puerto/", "categories": "linux", "tags": "linux", "date": "2025-12-29 18:25:00 +0100", "content": "Cuando queremos comprobar si hay conectividad contra un puerto TCP, he visto a mucha gente seguir utilizando todav√≠a telnet, llegando a instalarlo si no lo est√°. Esto me parece un poco una chapuza, cuando curl viene ya instalado por defecto en muchos sistemas operativos y cumple el mismo cometido. Para poder comprobar si hay conectividad contra un puerto, s√≥lo tendremos que ejecutar el siguiente comando: curl -v telnet://google.es:80 Donde: con -v indicaremos que el comando sea verbose, que muestre informaci√≥n. telnet que s√≥lo intente la conexi√≥n contra el puerto. google.es la direcci√≥n contra la que queremos comprobar si el puerto est√° abierto. :80 el puerto en cuesti√≥n. Si hay conexi√≥n nos mostrar√° el siguiente mensaje. curl -v telnet://google.es:80 * Host google.es:80 was resolved. * IPv6: 2a00:1450:4003:80d::2003 * IPv4: 142.250.200.99 * Trying 142.250.200.99:80... * Connected to google.es (142.250.200.99) port 80 Para terminar el comando s√≥lo tendremos que pulsar Ctrl+C, no como con telnet que te puedes volver loco. Si el puerto en cuesti√≥n no estuviera abierto, el mensaje que nos saldr√≠a ser√≠a este. curl -v telnet://192.168.1.1:81 * Trying 192.168.1.1:81... * connect to 192.168.1.1 port 81 from 192.168.1.57 port 33384 failed: Conexi√≥n rehusada * Failed to connect to 192.168.1.1 port 81 after 2 ms: Couldn't connect to server * Closing connection curl: (7) Failed to connect to 192.168.1.1 port 81 after 2 ms: Couldn't connect to server Con esto, ya habremos comprobado si tenemos conectividad contra el puerto que queramos. Si no tenemos conexi√≥n, ser√≠a conveniente revisar si el servidor est√° encendido, si el servicio que levanta ese puerto est√° en ejecuci√≥n o si tenemos un firewall que bloque√© el acceso. Espero que os haya gustado y os haya servido de ayuda. ¬°Hasta la pr√≥xima!" }, { "title": "Cabeceras b√°sicas Nginx", "url": "/posts/cabeceras-basicas/", "categories": "nginx", "tags": "nginx", "date": "2025-12-19 18:33:00 +0100", "content": "A la hora de configurar un nuevo dominio/subdominio, es conveniente que tenga la siguiente configuraci√≥n a nivel de cabeceras para reforzar la seguridad del sitio web. En este post, vamos a√±adir varias cabeceras a nuestro servidor Nginx para agregar m√°s seguridad, adem√°s de una breve explicaci√≥n de para qu√© vale cada cabecera. Configuraci√≥n b√°sica Esta configuraci√≥n puede ir en los bloques de configuraci√≥n de nginx de http, server y location. add_header X-Frame-Options \"SAMEORIGIN\" always; add_header X-Content-Type-Options \"nosniff\" always; add_header X-XSS-Protection \"1; mode=block\" always; add_header Permissions-Policy \"geolocation=self\" always; add_header Strict-Transport-Security \"max-age=31536000; includeSubDomains\" always; add_header Referrer-Policy \"strict-origin-when-cross-origin\" always; La opci√≥n always¬†es para que las cabeceras se env√≠en incluso en respuestas de error (como 404 o 500). Explicaci√≥n de cada cabecera X-Frame-Options \"SAMEORIGIN\": Evita que la web sea cargada dentro de un &lt;iframe&gt; de otro dominio, protegiendonos contra el clickjacking (que cliquemos algo que no veamos). X-Content-Type-Options \"nosniff\": Evita que JS se ejecute donde s√≥lo deber√≠a haber texto o im√°genes. X-XSS-Protection \"1; mode=block\": Este es √∫til para navegadores antiguos, donde si detecta un ataque XSS (una vulnerabilidad donde el ataque inyecta c√≥digo malicioso en web l√©gitimas), bloquea la p√°gina. Permissions-Policy \"geolocation=self\": S√≥lo permitimos que nuestro propio sitio pueda pedir la ubicaci√≥n del usuario, evitando que lo hagan scripts o iframes externos. Strict-Transport-Security \"max-age=31536000; includeSubDomains\": Fuerza que siempre se use HTTPS, aunque el usuario escriba http. Referrer-Policy \"strict-origin-when-cross-origin\": Controla informaci√≥n que se env√≠a en la cabecera Referer, indicando a otros dominios el dominio de d√≥nde viene la petici√≥n, no la URL completa. Comprobaci√≥n del nivel de seguridad Para comprobar si estas cabeceras se ha aplicado y nos aparezca un puntuaje de lo protegida que tenemos la web, utilizaremos la web https://securityheaders.com/. Aparte te mostrar√° abajo valores recomendados que debes agregar a tu web (las cuales son las que vienen m√°s arriba) Esta ser√≠a una web que no cuenta con seguridad (es un ejemplo): Esta ser√≠a una web que cuenta con seguridad: Cabecera Content-Security-Policy Esta cabecera no est√° incluida en este post, ya que requiere una configuraci√≥n espec√≠fica que hay que adaptar a cada dominio al que hay que agregarlo. Esta cabecera ofrece una capa de protecci√≥n adicional contra ataques como Cross-Site Scripting (XSS) o data injection. Espero que os haya gustado y os haya servido de ayuda. ¬°Hasta la pr√≥xima!" }, { "title": "Desactivar error multipath Ubuntu", "url": "/posts/desactivar-error-multipath/", "categories": "linux, ubuntu", "tags": "linux, ubuntu", "date": "2024-04-18 10:00:00 +0200", "content": "Trabajando con alg√∫n servidor Ubuntu, ya sea la versi√≥n 20.04 o 22.04, me he fijado que en los logs suele dar un error relacionado con multipath. A mi por lo menos me molesta mucho a la hora de tratar con el fichero /var/log/syslog, porque llena el log de l√≠neas que te distraen, por lo que en este post veremos c√≥mo quitar dichos mensajes. Lo primero que haremos es editar el fichero /etc/multipath.conf y dejaremos el fichero de la siguiente forma: defaults { user_friendly_names yes } blacklist { devnode \"^(ram|raw|loop|fd|md|dm-|sr|scd|st)[0-9]*\" devnode \"^sd[a-z]?[0-9]*\" } Donde en el segundo devnode pondremos sd[...] si corresponde con el nombre de nuestro disco, si tenemos otro, ponemos el que nos muestre el comando lsblk. Despu√©s de esto reiniciamos el servicio de multipath con sudo systemctl restart multipath-tools para aplicar los cambios que hemos hecho. Con todo esto, ya dejar√°n de salir esos dichosos mensajes. Espero que os haya gustado y os haya servido de ayuda. ¬°Hasta la pr√≥xima!" }, { "title": "Agregar disco a LVM", "url": "/posts/agregar-disco-lvm/", "categories": "linux, lvm", "tags": "linux, lvm", "date": "2024-04-01 10:00:00 +0200", "content": "En posts anteriores (ha pasado tiempo desde entonces, deber√≠a darle una vuelta y actualizarlo) vimos c√≥mo ampliar un disco LVM que ya ten√≠amos, as√≠ que en este post vamos a a√±adir un nuevo disco LVM de nuestro servidor Linux. Para empezar, vamos a a√±adir un disco en nuestro servidor utilizando nuestra herramienta de virtualizaci√≥n, ya sea VMware, VirtualBox, KVM, etc√©tera. Damos por hecho que sab√©is hacer esta parte, porque cada herramienta es un mundo. Una vez hecho esto, vamos al servidor en cuesti√≥n e iremos ejecutando los comandos que voy indicando, los cuales pondr√© con la explicaci√≥n de lo que hace cada uno: lsblk # Para listar los discos f√≠sicos, en mi caso es el disco /dev/vdb Vamos a usar la herramienta cfdisk hacer el particionado del disco, tambi√©n podemos usar fdisk, pero me resulta m√°s c√≥modo esta herramienta. Cada en el siguiente bloque de c√≥digo pondr√© las opciones a elegir en cada l√≠nea de comentario. sudo cfdisk /dev/vdb # Elegimos gpt # Seleccionamos [New] y el tama√±o de la partici√≥n, en mi caso 5G # Seleccionamos [Type] y buscamos \"Linux LVM\" # Seleccionamos ahora [Write] y confirmamos los cambios escribiendo \"yes\" # Cerramos el programa con [Quit] Una vez creado la partici√≥n del disco, vamos a crear la partici√≥n f√≠sica LVM, el grupo y la partici√≥n l√≥gica. sudo pvcreate /dev/vdb1 # Creamos la partici√≥n f√≠sica sudo vgcreate vg_test /dev/vdb1 # Creamos el grupo indicando la partici√≥n f√≠sica sudo lvcreate -n lv_test -l +100%FREE vg_test # Creamos la partici√≥n l√≥gica indicando el tama√±o del mismo y el grupo al que va asociado sudo lvchange -ay /dev/vg_test/lv_test # Activamos la partici√≥n f√≠sica. IMPORTANTE: esto s√≥lo se hace la primera vez que creamos la partici√≥n, para ampliarla no es necesario sudo mkfs.xfs /dev/mapper/vg_test-lv_test # Le damos formato, en este XFS sudo mkdir /data01/ # Creamos una carpeta donde montaremos el disco Lo pongo fuera del c√≥digo para remarcarlo. IMPORTANTE: esto s√≥lo se hace la primera vez que creamos la partici√≥n, para ampliarla no es necesario. Ahora vamos a configurar el fichero /etc/fstab para que el disco se monte autom√°ticamente en el servidor cada vez que arranque. echo \"/dev/mapper/vg_test-lv_test /data01/ xfs defaults 0 0\" | sudo tee -a /etc/fstab # Con esto a√±adiremos directamente la l√≠nea en el fichero sin tener que editarlo De momento no se ha montado la partici√≥n, si no queremos/tenemos que reiniciar el servidor, podemos ejecutar el comando sudo mount -a para que lo haga en caliente. Una vez hecho, si ejecutamos el comando df -h, ya veremos la partici√≥n LVM con el tama√±o del disco, el uso del mismo, lo que tiene disponible, su porcentaje y sobre qu√© carpeta est√° montado: [vagrant@freeipa-server01 ~]$ df -h Filesystem Size Used Avail Use% Mounted on devtmpfs 4.0M 0 4.0M 0% /dev tmpfs 886M 171M 716M 20% /dev/shm tmpfs 355M 9.5M 345M 3% /run /dev/mapper/rocky-root 125G 3.1G 122G 3% / /dev/vda1 1014M 202M 813M 20% /boot tmpfs 178M 0 178M 0% /run/user/1000 /dev/mapper/vg_test-lv_test 5.0G 68M 5.0G 2% /data01 [vagrant@freeipa-server01 ~]$ Con esto ya tendremos el nuevo disco en el servidor para darle el uso que queramos, separar las BBDD, almacenar los logs, etc√©tera. Espero que os haya gustado y os haya servido de ayuda. ¬°Hasta la pr√≥xima!" }, { "title": "Eliminar backups fallidos de Barman", "url": "/posts/eliminar-backup-fallidos-barman/", "categories": "linux, ubuntu, postgres, backup", "tags": "linux, ubuntu, postgres, backup", "date": "2024-03-14 09:00:00 +0100", "content": "Por si alg√∫n casual fallasen varios backups de seguidos, en vez de eliminarlos uno a uno podemos hacer lo siguiente para eliminar todos. Por si antes de eliminar los backups fallidos quieres saber cu√°les hay, tenemos que ejecutar el siguiente comando para que te muestre un listado de lo mismos, donde &lt;server&gt; es el servidor que queremos consultar. barman list-backups &lt;server&gt; | grep FAILED | awk '{print $2;}' Una vez identificados, podemos eliminarlos con el siguiente comando. for bad in $(barman list-backups &lt;server&gt; | grep FAILED | awk '{print $2;}'); do \\ barman delete &lt;server&gt; $bad \\ done Esto eliminar√° todos los backups fallidos de dicho servidor. Espero que os haya gustado y os haya servido de ayuda. ¬°Hasta la pr√≥xima!" }, { "title": "Restaurar backup de Barman", "url": "/posts/restaurar-backup-barman/", "categories": "linux, ubuntu, postgres, backup", "tags": "linux, ubuntu, postgres, backup", "date": "2024-02-12 09:00:00 +0100", "content": "En anteriores posts, instalamos y configuramos Barman para que se encargase de las copias de seguridad de nuestras BBDD montadas sobre PostgreSQL. En este veremos c√≥mo restaurar uno de los backups que hayamos hecho. Para ello seguiremos los siguientes pasos: Apagaremos el servidor Postgres que ser√° el destino, el comando de ejemplo que se utiliza es la versi√≥n 12 sobre un Ubuntu 22.04 systemctl stop postgresql@12-main.service Hacemos un backup de la carpeta de los datos originales en Postgres, aunque esto no es necesario si en el servidor no hay datos de por s√≠ (si es un servidor nuevo). Ejecutamos el siguiente comando desde Barman barman recover \\ --remote-ssh-command \"ssh postgres@&lt;ip_server&gt;\" \\ --target-time=\"2024-01-21 17:00:00.00+00:00\" \\ &lt;id_server&gt; &lt;backup_id&gt; &lt;ruta_destino&gt; Donde: &lt;ip_server&gt; es la IP del servidor destino, donde previamente hemos realizado la configuraci√≥n de las claves de SSH. --target-time es la fecha de cuando queremos recuperar el backup. He dejado esa fecha para saber el formato que tiene que tener. &lt;id_server&gt; el nombre del servidor que hemos definido en la configuraci√≥n del Barman, se puede obtener con el comando barman list-backup all para que te muestre todos los backups disponibles. Un ejemplo ser√≠a dbpgn1-pro. &lt;backup_id&gt; el timestamp del backup que tengamos disponible, tambi√©n se puede sacar con el comando barman list-backup all. Un ejemplo ser√≠a 20240215T200002. &lt;ruta_destino&gt; la carpeta donde vamos a dejar los datos en el servidor Postgres destino. Con esto ya podremos iniciar de nuevo el servidor Postgres y ya tendr√° los datos disponibles recuperados. systemctl start postgresql@12-main.service Este post m√°s corto de lo normal, pero era algo necesario de hacer, porque podemos tener los backups que queramos, pero si no sabemos c√≥mo utilizarlos, de poco nos sirve. Una pr√°ctica recomendable a realizar en nuestros entornos productivos es realizar cada cierto tiempo, semestralmente por ejemplo, un simulacro de restauraci√≥n, para comprobar que nuestros backups no est√°n corruptos el d√≠a que lo necesitemos realmente. Espero que os haya gustado y os haya servido de ayuda. ¬°Hasta la pr√≥xima!" }, { "title": "Crear una pol√≠tica de contrase√±as en FreeIPA", "url": "/posts/crear-politica-pass-freeipa/", "categories": "linux, ubuntu, rocky, freeipa", "tags": "linux, ubuntu, rocky, freeipa", "date": "2024-02-09 09:00:00 +0100", "content": "A la hora de crear usuarios que usaremos para servicios en nuestro FreeIPA, nos interesa que la contrase√±a de estos no cambien. As√≠ que lo que haremos en este post es crear una pol√≠tica que evite que caduquen. Crear grupo Antes de crear la pol√≠tica, vamos a crear un grupo donde ir√°n los usuarios que queramos que no se les cambie la contrase√±a cada cierto tiempo y al que asociaremos la pol√≠tica que crearemos m√°s adelante. En el apartado de Identidad &gt; Grupos de nuestro servidor crearemos un grupo, en mi caso lo llamar√© systemusers. El resto de par√°metros los dejaremos como est√°n, aunque pod√©is poner la descripci√≥n que quer√°is. Crear pol√≠tica contrase√±a Para ello nos iremos al apartado Pol√≠tica &gt; Pol√≠ticas de Contrase√±a. Ah√≠ a√±adiremos una nueva, con el bot√≥n a la derecha que pone Agregar, donde pondremos el grupo al que queremos asociar y una prioridad, en mi caso pondr√© 1, porque quiero que se aplique sobre las dem√°s. Guardamos dando a Agregar y Editar para continuar con la configuraci√≥n. En el siguiente pantallazo nos pedir√°n que pongamos la longitud m√≠nima de contrase√±a, n¬∫ de intentos fallidos y m√°s opciones, pero la que nos interesa es la opci√≥n de Vida m√°xima que es la que estipular√° que no caduque nuestra contrase√±a. Le pondremos el valor a 0 (cero) para que no expire, el resto de valores, pod√©is ponerlo como quer√°is realmente, al gusto del consumidor. Ya s√≥lo ser√° meter usuarios del sistema, o usuarios que no queramos que su contrase√±a caduque, al grupo que hemos creado anteriormente y tiene asociado la pol√≠tica que acabamos de crear. Espero que os haya gustado y os haya servido de ayuda. ¬°Hasta la pr√≥xima!" }, { "title": "Instalar cliente de FreeIPA", "url": "/posts/instalar-freeipa-cliente/", "categories": "linux, ubuntu, rocky, freeipa", "tags": "linux, ubuntu, rocky, freeipa", "date": "2024-02-06 09:00:00 +0100", "content": "En este post empezaremos a a√±adir servidores a nuestro dominio local, el cual desplegamos en el anterior post, para poder conectarnos a ellos mediante la autenticaci√≥n centralizada que nos ofrece IPA. Para este caso voy a usar de cliente otro Rocky Linux, pero dejar√© tambi√©n los comandos para instalarlo en un Ubuntu. Antes de empezar a hacer nada en el cliente, vamos a crear un usuario en FreeIPA que s√≥lo se utilice para unir hosts al dominio, ya que nos ayudar√° a nivel de seguridad y a la hora de la automatizaci√≥n. Creaci√≥n rol para a√±adir equipos al dominio Los siguiente pasos se hacen desde la UI del servidor de FreeIPA. Empezamos creando el rol mencionado con los permisos necesarios para poder unir los equipos al dominio y para eso iremos al apartado de Servidor IPA y en la pesta√±a que sale por defecto, Role-Based Access Control (o sus siglas RBAC), y agregamos uno al que llamaremos Host Enrollment. Al crearlo, daremos a Agregar y Editar para que nos lleve a la ventana donde podemos configurar los permisos. Vamos a la pesta√±a Objeto de servicio y Agregar. El permiso que vamos a utilizar es Host Administrators. Creaci√≥n usuario para a√±adir equipos al dominio Ahora vamos a la parte de crear el usuario, por lo que iremos a la parte de Identidad &gt; Usuarios para poder hacerlo, en mi caso llamar√© al usuario systemenrollment, y rellenamos los datos que nos solicitan. Tambi√©n daremos a Agregar y Editar para a√±adir ya el rol al usuario, aunque se puede hacer despu√©s sin ning√∫n problema pinchando sobre el nombre del usuario. Un consejo que os doy aqu√≠ es que pong√°is una contrase√±a cualquiera, acced√°is con el usuario, ya que os pedir√° cambiar la contrase√±a y ya ah√≠ le pon√©is la contrase√±a final que quer√°is. Dentro del apartado del usuario que acabamos de crear, pinchamos sobre la pesta√±a Roles y agregamos que tenga el rol Host Enrollment. A√±adir equipos al dominio Una vez tengamos nuestro usuario creado, lo siguiente es instalar el paquete necesario y ejecutar el comando para unirlo al dominio. Para instalarlo en Rocky Linux el comando ser√≠a: sudo dnf install freeipa-client -y Y para Ubuntu: sudo apt install freeipa-client -y La opci√≥n -y es para que se instale autom√°ticamente. Teniendo ya lo necesario instalado, tenemos que hacer unas configuraciones en el cliente antes de unir el equipo al dominio. El primero es a√±adir en /etc/hosts el dominio, quedando de la siguiente manera: 127.0.1.1 freeipa-server02.samurantech.local freeipa-server02 Y la otra configuraci√≥n a realizar, es cambiar para que el servidor DNS sea el servidor de FreeIPA. Con esto ya podemos ejecutar el comando que agregue el servidor al dominio, siendo el mismo para los distintos sistemas operativos Linux. sudo ipa-client-install --mkhomedir --enable-dns-update --principal=systemenrollment --password=&lt;PASSWORD&gt; -U &lt;PASSWORD&gt; es la contrase√±a que hab√©is puesto para dicho usuario. Ahora os explicar√© que significa cada par√°metro: --mkhomedir Crea la carpeta /home/ para los usuarios que inician sesi√≥n por primera vez. --enable-dns-update Permite que haga actualizaciones en el DNS cuando se detecta que la IP de la m√°quina cambie. --principal El usuario con el que vamos a unir el equipo. --password La contrase√±a del usuario. -U Para que la instalaci√≥n sea desatendida, que no requiera interactuaci√≥n con el usuario. Una vez hecho todo esto, ya nos aparecer√° en nuestro IPA para que podamos acceder con los usuarios que vayamos creando en √©l. Esto lo pod√©is ver en el apartado de Equipos, como muestro a continuaci√≥n. El nombre del equipo que he usado para este post es freeipa-server02.samurantech.local. Espero que os haya gustado y os haya servido de ayuda. ¬°Hasta la pr√≥xima!" }, { "title": "Instalar Patroni en Ubuntu", "url": "/posts/instalar-patroni/", "categories": "linux, ubuntu, postgres", "tags": "linux, ubuntu, postgres", "date": "2023-10-27 13:00:00 +0200", "content": "En este art√≠culo se documenta c√≥mo realizar la instalaci√≥n de Patroni, un componente para crear cluster de PostgreSQL con Zookeeper, etcd o Consul. En nuestro caso, utilizaremos etcd, que coment√© en el art√≠culo anterior, para ello y HAProxy, esto explicado al final de este mismo art√≠culo, para el balanceo entre los nodos finales. Previamente tendremos que haber instalado PostgreSQL en cada uno de los nodos en los que se vaya a desplegar Patroni. La instalaci√≥n se ha realizado sobre 3 nodos Ubuntu 22.04. Toda esta configuraci√≥n se est√° realizando con el usuario root Vamos a abrir los puertos necesarios en el firewall para que los nodos se puedan comunicar entre ellos y las aplicaciones que posteriormente conectemos contra el cl√∫ster: firewall-cmd --permanent --add-port={5432,8008}/tcp firewall-cmd --reload Configuramos Watchdog y los permisos necesarios. cat &lt;&lt;EOF | sudo tee /etc/udev/rules.d/99-watchdog.rules KERNEL==\"watchdog\", OWNER=\"postgres\", GROUP=\"postgres\" EOF echo \"softdog\" &gt;&gt; /etc/modules-load.d/softdog.conf modprobe softdog chown postgres: /dev/watchdog Tenemos que buscar si Softdog est√° en la blacklist de modules para que arranque con el servidor. grep blacklist /lib/modprobe.d/* /etc/modprobe.d/* |grep softdog # Nos aparecer√° el fichero en el que se encuentra, en nuestro caso es \"/lib/modprobe.d/blacklist_linux_5.4.0-148-generic.conf\", y eliminamos la l√≠nea \"blacklist softdog\" # Comprobamos que est√© cargado lsmod | grep softdog Instalamos Patroni en cada uno de los nodos. apt install -y patroni Ahora vamos a configurar el componente. De la siguiente manera s√≥lo tendremos que cambiar las IPs de los nodos de etcd a los que se tiene que conectar, propiedades tales como el nombre del host, se har√° autom√°ticamente. Con esto ser√° copiar y pegar. PGPORT=5432 CLUSTER_NAME=\"cluster-1\" MY_NAME=$(hostname --short) MY_IP=$(hostname -I | awk ' {print $1}') cat &lt;&lt;EOF | sudo tee /etc/patroni/config.yml scope: $CLUSTER_NAME namespace: /db/ name: $MY_NAME restapi: listen: \"0.0.0.0:8008\" connect_address: \"$MY_IP:8008\" authentication: username: patroni password: mySuperSecretPassword etcd3: hosts: - &lt;nodo1_etcd&gt;:2379 - &lt;nodo2_etcd&gt;:2379 - &lt;nodo3_etcd&gt;:2379 bootstrap: dcs: ttl: 30 loop_wait: 10 retry_timeout: 10 maximum_lag_on_failover: 1048576 postgresql: use_pg_rewind: true use_slots: true parameters: archive_mode: \"on\" archive_command: \"/bin/true\" logging_collector: 'on' initdb: - encoding: UTF8 - data-checksums - auth-local: peer - auth-host: scram-sha-256 pg_hba: - host replication replicator 0.0.0.0/0 scram-sha-256 - host all all 0.0.0.0/0 md5 # Some additional users which needs to be created after initializing new cluster users: admin: password: admin% options: - createrole - createdb postgresql: listen: \"0.0.0.0:$PGPORT\" connect_address: \"$MY_IP:$PGPORT\" data_dir: /data01/pgstorage/ bin_dir: /usr/lib/postgresql/15/bin/ pgpass: /tmp/pgpass0 authentication: replication: username: replicator password: confidential superuser: username: postgresql password: postgres rewind: username: rewind_user password: rewind_password parameters: unix_socket_directories: \"/var/run/postgresql/\" watchdog: mode: required device: /dev/watchdog safety_margin: 5 tags: nofailover: false noloadbalance: false clonefrom: false nosync: false EOF systemctl enable --now patroni systemctl restart patroni Si llegase a fallar la creaci√≥n autom√°tica de los usuarios, habr√≠a que crearlos o cambiarles la contrase√±a manual, meti√©ndonos con usuario postgres y ejecutando las siguientes queries desde psql. # Cambiar la contrase√±a del usuario postgres ALTER USER postgres WITH PASSWORD 'postgres'; # Crear el usuario para la replicaci√≥n CREATE ROLE replicator WITH REPLICATION LOGIN PASSWORD 'confidential'; # Crear el usuario para el rewind (el streaming de datos) y darle los permisos necesarios CREATE USER rewind_user LOGIN PASSWORD 'rewind_password'; GRANT EXECUTE ON function pg_catalog.pg_ls_dir(text, boolean, boolean) TO rewind_user; GRANT EXECUTE ON function pg_catalog.pg_stat_file(text, boolean) TO rewind_user; GRANT EXECUTE ON function pg_catalog.pg_read_binary_file(text) TO rewind_user; GRANT EXECUTE ON function pg_catalog.pg_read_binary_file(text, bigint, bigint, boolean) TO rewind_user; Para comprobar que la instalaci√≥n es correcta, con el siguiente comando podremos listar los nodos que tenemos, su rol, el lag a la hora de replicar entre ellos o si queda pendiente reiniciar alguno de los nodos por configuraciones que hayamos hecho. patronictl -c /etc/patroni/config.yml list + Cluster: cluster-1 (7284196933919984482) --------+----+-----------+--------------+ | Member | Host | Role | State | TL | Lag in MB | Tags | +----------------+---------------+--------------+-----------+----+-----------+--------------+ | n1-pro | 10.255.10.121 | Leader | running | 31 | | | | n2-pro | 10.255.10.122 | Replica | streaming | 31 | 0 | nosync: true | | n3-pro | 10.255.10.123 | Replica | streaming | 31 | 0 | nosync: true | +----------------+---------------+--------------+-----------+----+-----------+--------------+ HAProxy Con los pasos realizados anteriormente habremos configurado el cluster de Patroni, ahora configuraremos el HAProxy para que haga el balanceo. Previamente tendremos que haber instalado HAProxy, en este apartado s√≥lo se mencionar√° la configuraci√≥n. # Hacemos una copia del fichero de configuraci√≥n y eliminamos el contenido del original cp /etc/haproxy/haproxy.cfg{,.bak} &gt;/etc/haproxy/haproxy.cfg # Ponemos la configuraci√≥n correspondiente en el fichero cat&lt;&lt;EOF | tee /etc/haproxy/haproxy.cfg global maxconn 100 defaults log global mode tcp retries 2 timeout client 30m timeout connect 4s timeout server 30m timeout check 5s listen stats mode http bind *:7000 stats enable stats uri / listen read-write bind *:5432 option httpchk OPTIONS /read-write http-check expect status 200 default-server inter 3s fall 3 rise 2 on-marked-down shutdown-sessions server n1-pro 10.255.10.121:5432 maxconn 100 check port 8008 server n2-pro 10.255.10.122:5432 maxconn 100 check port 8008 server n3-pro 10.255.10.123:5432 maxconn 100 check port 8008 listen read-only balance roundrobin bind *:5433 option httpchk OPTIONS /replica http-check expect status 200 default-server inter 3s fall 3 rise 2 on-marked-down shutdown-sessions server n1-pro 10.255.10.121:5432 maxconn 100 check port 8008 server n2-pro 10.255.10.122:5432 maxconn 100 check port 8008 server n3-pro 10.255.10.123:5432 maxconn 100 check port 8008 EOF Reiniciamos el servicio de HAProxy y comprobamos en el log si todo est√° bien. systemctl restart haproxy; tail -f /var/log/haproxy.log Si nos conectamos a la IP del Haproxy y al puerto 7000, podremos ver que nodos est√°n en escritura/lectura y cu√°les en s√≥lo lectura. Con esto √∫ltimo ya habremos terminado la configuraci√≥n por completo. Se pueden hacer m√°s configuraciones como poner que uno de los nodos sea as√≠ncrono con la etiqueta nosync: true o s√≠ncrono si lo configuramos como nosync: false La verdad es que es una herramienta que da mucho juego y nos permite manejar un cluster de postgres de forma muy sencilla. Espero que os haya gustado y os haya servido de ayuda. ¬°Hasta la pr√≥xima!" }, { "title": "Instalar etcd en Ubuntu", "url": "/posts/instalar-etcd/", "categories": "linux, ubuntu, postgres", "tags": "linux, ubuntu, postgres", "date": "2023-10-24 17:00:00 +0200", "content": "En este art√≠culo voy a explicar c√≥mo realizar la instalaci√≥n de un cl√∫ster de ETCD, una BBDD que almacena los datos en formato key-value. La instalaci√≥n se ha realizado sobre 3 nodos Ubuntu 20.04. Toda esta configuraci√≥n se est√° realizando con el usuario root. Vamos a abrir los puertos necesarios en el firewall para que los nodos se puedan comunicar entre ellos y las aplicaciones que posteriormente conectemos contra el cl√∫ster: firewall-cmd --permanent --add-port={2379/tcp,2380/tcp} firewall-cmd --reload Creamos el usuario etcd, que ser√° quien arranque el servicio. Le hemos configurado para que no cree su carpeta home ni le d√© permisos que pueda abrir una shell, pues no lo necesita. useradd --no-create-home -s /bin/false etcd Creamos las carpetas necesarias y le damos permisos al usuario etcd mkdir -p /var/lib/etcd/default/ chown etcd: /var/lib/etcd/default/ Ahora vamos a descomprimir el fichero que contiene los ejecutables de etcd que anteriormente nos habremos descargado de la p√°gina de las releases Github de ETCD, en nuestro caso lo hemos dejado en /var/tmp. Una vez se ha descomprimido, movemos los ejecutables a la carpeta /usr/bin. tar xzvf /var/tmp/etcd-v3.4.27-linux-amd64.tar.gz mv etcd-v3.4.27-linux-amd64/etcd* /usr/bin/ Vamos a crear el fichero de servicio para etcd, con el siguiente comando no hace falta editar el fichero, nos deja ya el contenido que le pongamos en el mismo. cat &gt;/lib/systemd/system/etcd.service&lt;&lt;\\EOF [Unit] Description=etcd - highly-available key value store Documentation=https://github.com/coreos/etcd Documentation=man:etcd After=network.target Wants=network-online.target [Service] Environment=DAEMON_ARGS= Environment=ETCD_NAME=%H Environment=ETCD_DATA_DIR=/var/lib/etcd/default EnvironmentFile=-/etc/default/%p Type=notify User=etcd PermissionsStartOnly=true #ExecStart=/bin/sh -c \"GOMAXPROCS=$(nproc) /usr/bin/etcd $DAEMON_ARGS\" ExecStart=/usr/bin/etcd $DAEMON_ARGS Restart=on-abnormal #RestartSec=10s LimitNOFILE=65536 [Install] WantedBy=multi-user.target Alias=etcd2.service EOF Ahora vamos a crear el fichero de configuraci√≥n de etcd para el nodo 1. cat &gt;/etc/default/etcd&lt;&lt;\\EOF ETCD_NAME=node-1 ETCD_LISTEN_PEER_URLS=\"http://&lt;IP_nodo1&gt;:2380\" ETCD_LISTEN_CLIENT_URLS=\"http://&lt;IP_nodo1&gt;:2379\" ETCD_INITIAL_CLUSTER_TOKEN=\"etcd-cluster\" ETCD_INITIAL_CLUSTER=\"node-1=http://&lt;IP_nodo1&gt;:2380,node-2=http://&lt;IP_nodo2&gt;:2380,node-3=http://&lt;IP_nodo3&gt;:2380\" ETCD_INITIAL_ADVERTISE_PEER_URLS=\"http://&lt;IP_nodo1&gt;:2380\" ETCD_ADVERTISE_CLIENT_URLS=\"http://&lt;IP_nodo1&gt;:2379\" ETCD_INITIAL_CLUSTER_STATE=new EOF La configuraci√≥n para el nodo 2 ser√≠a la misma, pero cambiando lo siguiente. cat &gt;/etc/default/etcd&lt;&lt;\\EOF ETCD_NAME=node-2 ETCD_LISTEN_PEER_URLS=\"http://&lt;IP_nodo2&gt;:2380\" ETCD_LISTEN_CLIENT_URLS=\"http://&lt;IP_nodo2&gt;:2379\" ETCD_INITIAL_CLUSTER_TOKEN=\"etcd-cluster\" ETCD_INITIAL_CLUSTER=\"node-1=http://&lt;IP_nodo1&gt;:2380,node-2=http://&lt;IP_nodo2&gt;:2380,node-3=http://&lt;IP_nodo3&gt;:2380\" ETCD_INITIAL_ADVERTISE_PEER_URLS=\"http://&lt;IP_nodo2&gt;:2380\" ETCD_ADVERTISE_CLIENT_URLS=\"http://&lt;IP_nodo2&gt;:2379\" ETCD_INITIAL_CLUSTER_STATE=new EOF La configuraci√≥n para el nodo 3 ser√≠a la misma, pero cambiando lo siguiente. cat &gt;/etc/default/etcd&lt;&lt;\\EOF ETCD_NAME=node-3 ETCD_LISTEN_PEER_URLS=\"http://&lt;IP_nodo3&gt;:2380\" ETCD_LISTEN_CLIENT_URLS=\"http://&lt;IP_nodo3&gt;:2379\" ETCD_INITIAL_CLUSTER_TOKEN=\"etcd-cluster\" ETCD_INITIAL_CLUSTER=\"node-1=http://&lt;IP_nodo1&gt;:2380,node-2=http://&lt;IP_nodo2&gt;:2380,node-3=http://&lt;IP_nodo3&gt;:2380\" ETCD_INITIAL_ADVERTISE_PEER_URLS=\"http://&lt;IP_nodo3&gt;:2380\" ETCD_ADVERTISE_CLIENT_URLS=\"http://&lt;IP_nodo3&gt;:2379\" ETCD_INITIAL_CLUSTER_STATE=new EOF Lo que cambia de un fichero a otro son las propiedades que apuntan al nodo en el que estamos haciendo la configuraci√≥n. Para terminar, s√≥lo nos quedar√≠a aplicar los cambios en el daemon, habilitar el servicio y ver los logs. systemctl daemon-reload systemctl enable --now etcd.service journalctl -u etcd -f Con la opci√≥n enable --now hacemos que el servicio arranque con el sistema y lo inicia ahora. Con esto ya tendremos montado nuestro cl√∫ster de etcd, donde podremos comprobar los nodos y la salud del mismo con estos comandos. Donde &lt;IP_nodoN&gt; es cualquier nodo del cl√∫ster. etcdctl --endpoints http://&lt;IP_nodoN&gt;:2379 member list --write-out=table etcdctl --endpoints http://&lt;IP_nodoN&gt;:2379 endpoint status --cluster --write-out=table La verdad es que s√≥lo conoc√≠a esta BBDD de o√≠das porque es con la que trabaja Kubernetes, y ahora entiendo por qu√© la utiliza. Es muy ligera y la funcionalidad que tiene es impresionante, es mi caso la uso para integrar servicios como Patroni, que har√© un art√≠culo m√°s adelante, APISIX u otro servicio. Espero que os haya gustado y os haya servido de ayuda. ¬°Hasta la pr√≥xima!" }, { "title": "Instalar Barman para Ubuntu", "url": "/posts/instalar-barman/", "categories": "linux, ubuntu, postgres, backup", "tags": "linux, ubuntu, postgres, backup", "date": "2023-05-02 10:00:00 +0200", "content": "Tener al d√≠a los backups es algo muy importante y m√°s si hablamos de BBDD, por eso el d√≠a de hoy traigo esta herramienta llamada Barman que funciona con servidores PostgreSQL. Para este tutorial voy a contar con que ya tengamos un servidor de PostgreSQL montado, para centrarnos √∫nicamente en la instalaci√≥n de Barman y la configuraci√≥n que hay que realizar en ambos servidores. Instalaci√≥n Barman y barman-cli Para instalarlo es muy sencillo, s√≥lo tendremos que ejecutar el siguiente comando en el servidor de Barman. sudo apt update -y &amp;&amp; sudo apt install barman -y Con esto actualizaremos los repositorios en Ubuntu e instalaremos el paquete sin necesidad de tener que confirma. Una vez haya terminado, podremos ponernos a editar los ficheros de configuraci√≥n, que se pueden encontrar bien en /etc/barman.conf o /etc/barman/barman.conf. Para comprobar que versi√≥n tenemos instalada, podemos ejecutar barman --version Para el servidor de PostgreSQL tendremos que instalar barman-cli de la siguiente manera. sudo apt update -y &amp;&amp; sudo apt install barman-cli -y Configuraci√≥n Barman En este apartado vamos a hacer la parte tanto que implica el servidor de Barman como en el servidor PostgreSQL. Autenticaci√≥n sin contrase√±a en Barman Lo primero que hay que hacer es iniciar sesi√≥n con el usuario barman y generar un par de claves para la conexi√≥n sin contrase√±a por SSH. sudo su barman ssh-keygen -t rsa De esta forma, no tendremos que estar cambiando la contrase√±a del usuario barman. En el segundo comando, damos siguiente a todo y nos habr√° creado el par de claves. Para pasarlos al otro servidor, podemos ejecutar el comando ssh-copy-id pero en ocasiones no tenemos la contrase√±a del usuario postgres, as√≠ que yo lo har√© a mano, copiando el contenido del fichero /var/lib/barman/.ssh/id_rsa.pub y dej√°ndolo en el fichero authorized_keys que crearemos cuando configuremos la parte de postgres. Autenticaci√≥n sin contrase√±a en PostgreSQL Tendremos que hacer lo mismo que hemos realizado en el paso anterior, pero s√≥lo que habr√° que iniciar sesi√≥n con el usuario postgres en el servidor de PostgreSQL. sudo su postgres ssh-keygen -t rsa De igual manera, damos siguiente a todo para que nos cree el par de claves. Por defecto nos dejar√° las claves en /var/lib/postgres/.ssh/id_rsa.pub En este punto tendremos que hacer lo siguiente, el contenido del fichero /var/lib/postgres/.ssh/id_rsa.pub del servidor PostgreSQL tendr√° que estar en /var/lib/barman/.ssh/authorized_keys y el fichero /var/lib/barman/.ssh/id_rsa.pub en /var/lib/postgres/.ssh/authorized_keys. Con esto podremos hacer que ambos servidores se conecten el uno al otro sin necesidad de poner contrase√±a. Algo recomendable por si os salta error es intentar conectarse por ssh del servidor de Barman con el usuario barman al servidor de PostgreSQL y del servidor de PostgreSQL con el usuario postgres al servidor de barman para aceptar la key fingerprint. Crear rol barman en BBDD Necesitamos crear un usuario con permisos de superusuario para poder hacer el backup. Para ello, nos conectaremos a nuestra BBDD, ya sea con herramientas como PGAdmin o la propia l√≠nea de comandos, psql. Os dejo el comando a ejecutar para crearlo. La contrase√±a puede ser la que quiera, que se indica en lo entrecomillado que viene despu√©s de password. postgres=# create user barman superuser password 'barman'; Modificar ficheros configuraci√≥n Barman En este apartado vamos a cambiar tanto el fichero general de Barman, como el fichero que hay que crear para la conexi√≥n al servidor PostgreSQL. En el fichero de configuraci√≥n /etc/barman.conf, vamos a a√±adir las siguientes l√≠neas, ya sea agreg√°ndolas o descomentando las que ya hay. barman_home = /var/lib/barman # Aqu√≠ definimos d√≥nde se guardan los backups compression = gzip parallel_jobs = 2 # Aqu√≠ definimos que a la hora de hacer la copia, utilic√© 2 jobs en vez de 1 como hace por defecto retention_policy = RECOVERY WINDOW OF 4 WEEKS # Aqu√≠ definimos que queremos que mantengan 4 semanas de backups retention_policy_mode = auto Para el fichero de conexi√≥n al servidor que queramos hacer el backup, dentro de la carpeta /etc/barman/conf.d hay templates para poder copiar el fichero y simplemente modificar los datos de conexi√≥n. Aqu√≠ os dejo uno de ejemplo. ; Barman, Backup and Recovery Manager for PostgreSQL ; http://www.pgbarman.org/ - http://www.2ndQuadrant.com/ ; ; Template configuration file for a server using ; SSH connections and rsync for copy. ; [postgres-test] ; Human readable description description = \"Servidor test Postgres\" ; ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;; ; SSH options (mandatory) ; ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;; ssh_command = ssh -q postgres@10.1.10.201 ; ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;; ; PostgreSQL connection string (mandatory) ; ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;; conninfo = host=10.1.10.201 user=barman dbname=postgres ; ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;; ; Backup settings (via rsync over SSH) ; ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;; backup_method = rsync ; Incremental backup support: possible values are None (default), link or copy reuse_backup = link ; Identify the standard behavior for backup operations: possible values are ; exclusive_backup (default), concurrent_backup ; concurrent_backup is the preferred method with PostgreSQL &gt;= 9.6 backup_options = exclusive_backup ; Number of parallel workers to perform file copy during backup and recover parallel_jobs = 2 ; ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;; ; Continuous WAL archiving (via 'archive_command') ; ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;; archiver = on ;archiver_batch_size = 50 ; PATH setting for this server path_prefix = \"/usr/pgsql-12/bin\" retention_policy = RECOVERY WINDOW OF 2 WEEKS retention_policy_mode = auto Comprobar conexi√≥n Barman-PostgreSQL Una vez hecho todo esto, podremos verificar que Barman ha tomado correctamente la configuraci√≥n que hemos realizado y se conecta al servidor. Con barman list-server, nos mostrar√° los servidores que hayamos configurado. Con barman show-server &lt;server&gt;, en este caso cambiamos &lt;server&gt; por postgres-test como he puesto en el fichero de configuraci√≥n y nos mostrar√° la configuraci√≥n para ese servidor. Con el siguiente comando comprobaremos la conexi√≥n, que en este punto nos dar√° fallo porque todav√≠a queda por configurar los WAL. El comando es cuesti√≥n es barman check &lt;server&gt; Configurar env√≠o WAL a servidor Barman Hay que modificar los siguientes par√°metros en el fichero de configuraci√≥n postgresql.conf que se encuentra en el servidor PostgreSQL. archive_mode = on # Prestar especial atenci√≥n a este, porque si est√° con otro valor, habr√° que reiniciar el servicio archive_command = 'barman-wal-archive 10.1.10.202 10.1.10.201 \"%p\" ' # Siendo la primera IP el servidor de Barman y la segunda el servidor de PostgreSQL. Con este s√≥lo ser√° necesario hacer un reload Reiniciamos o recargamos el servicio de PostgreSQL seg√∫n venga al caso y volvemos al servidor de Barman. En el servidor de Barman ejecutaremos el siguiente comando para forzar que copie los ficheros WALs y comprobar que hemos hecho la configuraci√≥n correctamente. barman switch-xlog --force --archive postgres-test Volvemos a ejecutar el comando barman check postgres-test para confirmar que va a realizar el backup correctamente. Configuraci√≥n extra Para que Barman haga una comprobaci√≥n del estado de los servidores y se traiga los WALs que estos tengan, hay que hacer una configuraci√≥n en Crontab del usuario barman. sudo su barman crontab -e Ah√≠ podremos configurar que cada minuto haga la comprobaci√≥n y haga un backup, ya sea completo o incremental seg√∫n indiquemos. Aqu√≠ un ejemplo de ello. */1 * * * * barman cron # Cada minuto har√° la comprobaci√≥n que he comentado 00 22 * * 1 barman backup --reuse-backup off postgres-test # Que haga una copia completa los lunes, a las 10 de la noche 00 22 * * 0,2,3,4,5,6 barman backup postgres-test # Que haga una copia incremental el resto de d√≠as, a las 10 de la noche Con todo esto, ya tendremos un backup de nuestros servidores de BBDD, por si fuera necesario alg√∫n tener que echar mano de ellos, que esperemos que no. Espero que os haya gustado y os haya servido de ayuda. ¬°Hasta la pr√≥xima!" }, { "title": "Instalar FreeIPA Server en Rocky Linux 9", "url": "/posts/instalar-freeipa-server-rockylinux/", "categories": "linux, rocky, freeipa", "tags": "linux, rocky, freeipa", "date": "2023-04-01 10:00:00 +0200", "content": "Para que nos entendamos con FreeIPA, es como el Active Directory de Windows, pero desde mi punto de vista, mejor porque es en Linux. Con este servicio podremos centralizar la autenticaci√≥n y autorizaci√≥n a los servidores de nuestra infraestructura, adem√°s de poder gestionar certificados, claves SSH (aunque est√© incluido en lo que he comentado en lo de autenticaci√≥n/autorizaci√≥n), poder hacer que se automonte particiones en los servidores, DNS, etc√©tera‚Ä¶ Vamos que vale para muchas cosas que ya iremos viendo. He elegido Rocky Linux 9 por dos motivos, para Ubuntu s√≥lo hay hasta la versi√≥n 18.04 y porque este es el sucesor espiritual de CentOS. Prerequisitos Primero que todo vamos a actualizar los repositorios y paquetes del servidor que hayamos desplegado para ello con el siguiente comando: sudo dnf update -y Ahora vamos a asegurarnos que la fecha y la hora est√° bien, adem√°s de configurar el nombre del servidor, que tenga una IP est√°tica. En el ejemplo he puesto los datos de mi servidor, pero vosotros tendr√©is que cambiar la IP y el nombre para que corresponda con el vuestro. # Cambiar la zona horaria sudo timedatectl set-timezone Europe/Madrid # Cambiar el nombre del host a√±adiendo el nombre del dominio sudo hostnamectl set-hostname freeipa-server1.samurantech.local sudo sh -c 'echo \"192.168.122.11 freeipa-server1.samurantech.local freeipa-server1\" &gt;&gt; /etc/hosts' # Comprobar que los cambios anteriores se hayan efectuado timedatectl &amp;&amp; hostnamectl Si en el fichero /etc/hosts hay alguna referencia al servidor con la IP 127.0.1.1 o con IPv6, yo recomiendo eliminarlo para que no d√© problemas. Instalaci√≥n FreeIPA Server Ahora s√≠ que vamos a empezar con la instalaci√≥n como tal, por lo que tendremos que ejecutar el siguiente comando para que instale los paquetes necesarios, el servidor, los DNS y el cliente de FreeIPA. sudo dnf install freeipa-server freeipa-server-dns freeipa-client -y Una vez terminado, vamos a configurarlo. Configuraci√≥n FreeIPA Server Para instalarlo podr√≠amos simplemente ejecutar el comando ipa-server-install e ir rellenando los datos que nos vayan pidiendo, pero para hacerlo m√°s c√≥modo vamos a ponerle los siguientes par√°metros para que lo configure de una sola vez. sudo ipa-server-install --ip-address=192.168.122.11 \\ --realm SAMURANTECH.LOCAL \\ --ds-password=DS_PASS \\ --admin-password=ADMIN_PASS \\ --setup-dns \\ --auto-reverse \\ --forwarder 8.8.8.8 \\ --unattended Ahora os explicar√© que significa cada par√°metro: --ip-address Selecciona la IP por d√≥nde queremos que el servidor levante los servicios, si tienes varias IPs en un mismo servidor como es mi caso, es mejor especificar. --realm C√≥mo queremos que se llame nuestro dominio de Kerberos, lo recomiendo poner en may√∫sculas. --ds-password La contrase√±a del Directory Server para el usuario del Directory Manager. Tiene que tener un m√≠nimo de 8 caracteres. --admin-password La contrase√±a del usuario administrador. Tiene que tener un m√≠nimo de 8 caracteres. --setup-dns Para configurar el servidor DNS seg√∫n hace la configuraci√≥n. --auto-reverse Para que cree autom√°ticamente la zona reversa DNS con sus registros PTR. --forwarder Configurar el reenviador DNS. --unattended Para que no tenga que pedirnos interactuar con la instalaci√≥n, que lo haga autom√°ticamente. A√±adir que se puede configurar un CA externo, pero si no se especifica, FreeIPA crea uno propio. Si os da un error con IPv6 como fue mi caso, tenemos que a√±adir/modificar en el fichero /etc/sysctl.conf la l√≠nea net.ipv6.conf.all.disable_ipv6 = 0 (si veis un 1, hay que poner un 0) y ejecutar sudo sysctl -p para que aplique los cambios sin tener que reiniciar el servidor. Si tenemos firewalld ejecut√°ndose en nuestro sistema, tendremos que abrir los puertos necesarios para poder acceder: sudo firewall-cmd --add-service={freeipa-4,dns,ntp} --permanent sudo firewall-cmd --add-port={88/udp,464/udp,464/tcp} --permanent sudo firewall-cmd --reload El servicio freeipa-4 tiene incluido todos los puertos necesarios como HTTPS, LDAP, LDAPS, Kerberos y Kpasswd (Para m√°s informaci√≥n, puedes mirarlo en /usr/lib/firewalld/services/freeipa-4.xml) Comprobar que el servicio funciona Verificamos que los servicios de IPA est√°n corriendo: [samuel@freeipa-server1 ~]$ sudo ipactl status Directory Service: RUNNING krb5kdc Service: RUNNING kadmin Service: RUNNING named Service: RUNNING httpd Service: RUNNING ipa-custodia Service: RUNNING pki-tomcatd Service: RUNNING ipa-otpd Service: RUNNING ipa-dnskeysyncd Service: RUNNING ipa: INFO: The ipactl command was successful Tambi√©n podemos conectarnos por https para acceder a su interfaz web, en mi caso https://freeipa-server1.samurantech.local/ipa/ui/ En los siguientes posts veremos c√≥mo crear usuarios desde la web, c√≥mo a√±adir un cliente al nuestro dominio y crear un segundo servidor para que haya replicaci√≥n maestro-maestro, entre otros. Espero que os haya gustado y os haya servido de ayuda. ¬°Hasta la pr√≥xima!" }, { "title": "Instalar Let's Encrypt en CentOS 7 y Ubuntu 20.04", "url": "/posts/instalar-letencrypt/", "categories": "linux, centos, ubuntu", "tags": "linux, centos, ubuntu", "date": "2023-03-18 13:00:00 +0100", "content": "Configurar HTTPS para cifrar comunicaciones es algo muy importante y con esta herramienta podremos conseguir nuestros propios certificados firmados y gratuitos. Let‚Äôs Encrypt para quien no lo sepa, es una CA (Autoridad certificadora) gratuita, automatizada y abierta que nos permite crear lo comentado anteriormente. Importante, esto s√≥lo es v√°lido si tenemos un dominio p√∫blico para que Let‚Äôs Encrypt pueda comprobar nuestra identidad y un registro DNS publicado, no funciona si queremos trabajar con localhost. Dependencias para CentOS 7 S√≥lo indico para CentOS ya que para Ubuntu no es necesario instalar nada previamente, actualizar el sistema con apt update y ya. Actualizamos e instalamos dependencias necesarias. Adem√°s de asegurarnos que tenemos disponible epel-release. yum update -y yum -y install yum-utils En la Wiki de CentOS te indican c√≥mo activarlo en arquitecturas ARMHFP por si trabaj√°is en una Raspberry Pi. Una vez hecho esto, podemos seguir con la instalaci√≥n. Instalaci√≥n Certbot Instalamos Certbot. # Para CentOS yum install certbot # Para Ubuntu apt install certbot Comprobamos que se ha instalado correctamente ejecutando certbot --version y que nos muestre la versi√≥n del programa. Creaci√≥n de los certificados Para obtener √∫nicamente el certificado, ejecutaremos el siguiente comando, reemplazando lo que viene a continuaci√≥n de -d por nuestro dominio. Aqu√≠ pondremos algunos par√°metros m√°s, porque el comando puede llegar a ser un poco pu√±etero si dejamos los valores por defecto. certbot certonly --manual --preferred-challenges=dns --email=samuran@samurantech.com --agree-tos -d *.samurantech.com Donde: certonly, indicamos que queremos descargar el certificado, que nos proporcione los ficheros. --manual, hacemos que la ejecuci√≥n sea de forma interactiva, que nos permita realizar cambios en los registros DNS, por ejemplo, antes de continuar con la ejecuci√≥n del comando. --preferred-challenges=dns, recomiendo esta opci√≥n porque s√≥lo tendremos que crear un registro TXT en nuestro proveedor de DNS para que el comando compruebe que tenemos en propiedad ese dominio. Hay otra opci√≥n cambiando dns por http, pero la considero m√°s tediosa al tener que crear un fichero en un servidor, publicarlo a internet y el comando compruebe si existe. --email=samuran@samurantech.com, la cuenta de correo donde llegar√°n las notificaciones importantes. --agree-tos, aceptamos las condiciones de uso y t√©rminos. -d &lt;dominio&gt;, dominio del que queremos obtener un certificado. Con * indicamos que sea un wildcard (comod√≠n para subdominios). Cuando lo ejecutemos nos aparecer√° un registro TXT que tendremos que crear (os generar√° un valor aleatorio, yo he puesto cualquier valor para el ejemplo): Please deploy a DNS TXT record under the name _acme-challenge.samurantech.com with the following value: xxxxxxxxxxxxxxxxxxxxxxxx-xxxxxxxxxxxxxxxxxx Before continuing, verify the record is deployed. Una vez creado el registro TXT en nuestro proveedor DNS, daremos a Enter y los servidores de Let‚Äôs Encrypt se encargar√°n de comprobar que eso es as√≠. Con esto tendremos todos los ficheros necesarios en la carpeta /etc/letsencrypt/live/&lt;dominio&gt;/. Y como √∫ltimo, no olvidar configurar que los certificados se renueven autom√°ticamente creando una tarea en cron con crontab -e: 00 5 * * * /usr/bin/certbot renew Esto ejecutar√° la tarea todos los d√≠as a las 5 de la ma√±ana. Espero que os haya gustado y os haya servido de ayuda. ¬°Hasta la pr√≥xima!" }, { "title": "Configurar Catalina.out en Tomcat9", "url": "/posts/configurar-catalina-tomcat9/", "categories": "linux, tomcat", "tags": "linux, tomcat", "date": "2023-03-11 10:30:00 +0100", "content": "Por defecto, Tomcat 9 crea dicho fichero con la fecha al final del nombre (catalina.2022-08-30.log). Para que cree un √∫nico fichero catalina.out hay que modificar el fichero /lib/systemd/system/tomcat9.service (si lo hemos instalado por repositorio) a√±adiendo las siguientes l√≠neas al final del mismo. # Logging StandardOutput=append:/var/log/tomcat9/catalina.out StandardError=append:/var/log/tomcat9/catalina.out Una vez hecho esto, ejecutamos systemctl daemon-reload para que coja los cambios al haber modificado el fichero del servicio y aunque a veces no es necesario porque lo hace autom√°tico, reiniciar el servicio de Tomcat9 con systemctl restart tomcat9. Espero que os haya gustado y os haya servido de ayuda. ¬°Hasta la pr√≥xima!" }, { "title": "Configurar inotify", "url": "/posts/configurar-inotify/", "categories": "linux", "tags": "linux", "date": "2023-03-02 10:30:00 +0100", "content": "Inotify es un watcher que vigila un directorio y cuando detecta un evento que nosotros definamos, como que llegue un fichero nuevo (o movido en √©l) ejecuta una acci√≥n. Esto resulta muy √∫til si queremos que un proceso se ejecute al momento en vez de esperar a cierta hora para que eso se produzca. Ejemplo de configuraci√≥n #!/bin/bash source /etc/scripts/.variables inotifywait -m /home/test/in -e close_write -e moved_to | while read directory action file; do if [[ \"$file\" == *.csv ]]; then curl -X POST -u usuario:$TOKEN_EJEC https://jenkins.samurantech.com:8443/job/batch-procesa-ficheros/buildWithParameters?token=$TOKEN_EJEC -F fileInPath=/in/$file fi done El inotify se queda escuchando en el directorio home/test/in a la espera de un nuevo fichero y cuando termina de copiarse uno con la extensi√≥n indicada, en este caso .csv, llama al job de Jenkins que hayamos definido (los pr√≥ximos art√≠culos meteremos mano a Jenkins), pas√°ndole como par√°metro la ruta y el nombre del fichero. Se le pasa tambi√©n como variables el token para la ejecuci√≥n del job en un fichero externo, el cual se guarda en el fichero /etc/scripts/.variables Vamos a guardar el script anterior en un fichero, darle permisos de ejecuci√≥n con chmod +x &lt;fichero_script&gt;.sh y crearemos un servicio para que se quede ejecutando en segundo plano y arranque junto al sistema. Creaci√≥n fichero servicio Creamos un fichero en /etc/systemd/system/ con el nombre que queramos terminado en .service, en mi caso batch-procesa-ficheros.service. [Unit] Description=Batch procesa ficheros After=network.target [Service] ExecStart=/var/opt/scripts/batch-procesa-ficheros.sh [Install] WantedBy=default.target Ejecutamos los siguientes comandos para recargar el daemon de Linux y habilitar el servicio creado, adem√°s de arrancarlo en el mismo comando. systemctl daemon-reload &amp;&amp; systemctl enable batch-procesa-ficheros --now Espero que os haya gustado y os haya servido de ayuda. ¬°Hasta la pr√≥xima!" }, { "title": "Configurar HTTPS en Tomcat", "url": "/posts/configurar-https-tomcat/", "categories": "linux, tomcat, https", "tags": "linux, tomcat, https", "date": "2023-02-28 19:00:00 +0100", "content": "En el art√≠culo anterior habl√© de un error al intentar arrancar Tomcat9 y ca√≠ en que podr√≠a hablar de c√≥mo configurar HTTPS en este servicio. Usaremos certificados PEM, en vez de JKS, ya que pienso que es m√°s c√≥modo al no tener que generar dicho fichero para esto y que la contrase√±a se tiene que dejar en el fichero de configuraci√≥n en texto plano. Configuraci√≥n Para que Tomcat trabaje con HTTPS, en este caso utilizando el puerto 8443, hay que realizar la siguiente configuraci√≥n, tanto a nivel de los ficheros de Tomcat, como los permisos m√≠nimos que hay que configurar en los certificados. Hay que descomentar y configurar la ruta de los certificados del siguiente bloque de c√≥digo que se encuentra en /etc/tomcat/server.xml. En este caso, hemos predefinido que esa sea la ruta donde se almacenen los certificados. En el propio certificado vienen incluido el CA Root y CA Intermedio. &lt;Connector port=\"8443\" protocol=\"org.apache.coyote.http11.Http11AprProtocol\" maxThreads=\"150\" SSLEnabled=\"true\"&gt; &lt;UpgradeProtocol className=\"org.apache.coyote.http2.Http2Protocol\" /&gt; &lt;SSLHostConfig&gt; &lt;Certificate certificateKeyFile=\"/etc/pki/tls/private/private_key.key\" certificateFile=\"/etc/pki/tls/certs/certificate.chained.crt\" type=\"RSA\" /&gt; &lt;/SSLHostConfig&gt; &lt;/Connector&gt; Los certificados tienen que tener ciertos permisos para que Tomcat pueda trabajar con ellos, pero que no sea accesible para cualquiera. La clave p√∫blica puede tener permisos 644 y que el propietario sea root. La clave privada tenemos que ponerle permisos 400 tanto al usuario root como al usuario tomcat. Para configurar que otro usuario tenga permisos sobre un fichero, podemos configurarlo con el comando sudo setfacl -m u:tomcat:4 private_key.key Espero que os haya gustado y os haya servido de ayuda. ¬°Hasta la pr√≥xima!" }, { "title": "Tomcat9 no inicia al reiniciar el servidor", "url": "/posts/tomcat9-no-inicia/", "categories": "linux, tomcat", "tags": "linux, tomcat", "date": "2023-02-27 15:30:00 +0100", "content": "Donde trabajo estamos migrando los servidores con Tomcat 8 a Tomcat 9 y al reiniciar el servidor por completo, nos encontramos con esto, que el servicio no se levanta autom√°ticamente, pero si lo inicias mediante un systemctl restart tomcat9, levanta sin problemas. Para dar m√°s detalles, esto me ocurri√≥ en Ubuntu, con la versi√≥n de Tomcat 9 instalada por repositorio. Soluci√≥n El fallo da porque hay que configurar en el fichero del servicio /lib/systemd/system/tomcat9.service el siguiente par√°metro: ReadWritePaths=/usr/libexec/tomcat9. Recargamos el daemon para que tome los cambios que hemos realizado en el servicio y a la pr√≥xima no vuelva a ocurrir. Esto ocurre porque hay que permitir que pueda leer de esa ruta, donde se encuentra varios scripts para arrancar tomcat9 con el sistema. Espero que os haya gustado y os haya servido de ayuda. ¬°Hasta la pr√≥xima!" }, { "title": "Configurar relay Postfix", "url": "/posts/configurar-relay-postfix/", "categories": "linux", "tags": "linux", "date": "2023-02-26 17:50:00 +0100", "content": "En ocasiones nos interesa que nuestro servidor de correo Postfix utilice un SMTP relay para mandar los emails por otro lado, ya sea por motivos de seguridad o por casos temporales como puede ser en mi caso porque Apple se puso tonto y met√≠an cada dos por tres a mis dominios en sus blacklists. Configuraci√≥n Esta configuraci√≥n es para que los correos de ciertos dominios, icloud.com y me.com, sean mandados por el relay de GMail. En el fichero /etc/postfix/main.cf hay que agregar la siguiente l√≠nea. transport_maps = hash:/etc/postfix/bysender En el fichero mencionado, /etc/postfix/bysender agregamos los dominios que queremos que se mande por el relay de la siguiente manera: icloud.com smtp:[smtp-relay.gmail.com]:25 me.com smtp:[smtp-relay.gmail.com]:25 Ejecutamos el siguiente comando para que Postfix cree el fichero correspondiente y posteriormente, recargamos el servicio. postmap /etc/postfix/bysender systemctl reload postfix Con esto, cada vez que un correo de los dominios configurados, se mandar√°n por el relay definido. En este caso, hay que tener ciertas consideraciones con el relay de GMail, como el limite de correos que se pueden mandar. Info Espero que os haya gustado y os haya servido de ayuda. ¬°Hasta la pr√≥xima!" }, { "title": "Obtener hash de un fichero (Windows/Linux)", "url": "/posts/obtener-hash-windows-linux/", "categories": "linux, windows", "tags": "linux, windows", "date": "2023-01-29 18:55:00 +0100", "content": "Una cosa muy importante a tener en cuenta cuando descargamos un fichero de internet, es asegurar su integridad, que los datos no hayan sido modificados sin autorizaci√≥n en dicho proceso. Para ello, lo recomendable es comprobar el hash del mismo de las siguientes formas, ya sea para Windows o Linux. Tambi√©n es recomendable comprobar el hash cuando se copia un fichero de un equipo a otro, ya que si hay alg√∫n corte en la comunicaci√≥n, el fichero se queda corrupto. Windows Para empezar, nos iremos al Explorador de archivos en Windows 10 y a la ruta donde se encuentre el fichero que queramos comprobar. En la barra de direcciones, escribiremos powershell para abrirlo. Ejecutamos el siguiente comando y as√≠ obtendremos el hash del fichero, en este caso en SHA256, donde &lt;fichero&gt; es el fichero que queremos comprobar. Get-FileHash -Algorithm SHA256 &lt;fichero&gt; Normalmente en las webs te suele venir una secci√≥n donde te muestre el hash o un fichero con √©l. Linux En Linux tenemos que ir al directorio donde se encuentra el fichero y ejecutamos lo siguiente. sha256sum &lt;fichero&gt; Aunque sea un proceso sencillo, hay que tomar la costumbre de comprobar el hash de los ficheros para que no nos cuelen un fichero malicioso o nos volvamos locos intentando instalar un componente y no podamos porque el fichero se haya quedado corrupto por quedarse a medio copiar (cosa que me ha pasado en m√°s de una ocasi√≥n). Espero que os haya gustado y os haya servido de ayuda. ¬°Hasta la pr√≥xima!" }, { "title": "Utilizar jekyll en Docker", "url": "/posts/jekyll-docker/", "categories": "docker, jekyll", "tags": "docker, jekyll", "date": "2023-01-22 19:55:00 +0100", "content": "Para hacer esta web, utilic√© Jekyll, un generador de sitios web est√°ticos que permite crear cada p√°gina web a partir de un fichero Markdown. Para que sea mucho m√°s c√≥modo utilizarlo, usaremos Docker para que sea s√≥lo usar un contenedor y no tener que instalar m√°s cosas en nuestros equipos. Crear sitio web Ejecutaremos el siguiente comando para que Docker cree el contenedor y ejecute dentro del mismo el comando que utiliza Jekyll para generar lo que vendr√° a ser nuestro blog. export blog_name=\"blog-test\" docker run --rm --volume \"$PWD:/srv/jekyll\" -it jekyll/jekyll jekyll new $blog_name &amp;&amp; cd $blog_name El par√°metro --rm indica que una vez se ejecuta el contenedor, tiene que eliminarlo. Con -it permite generar una pseudo-terminal, lo que posibilita ejecutar comandos mientras el contenedor se ejecuta. Nos habr√° creado la siguiente estructura de directorios. blog-test ‚îú‚îÄ‚îÄ 404.html ‚îú‚îÄ‚îÄ about.markdown ‚îú‚îÄ‚îÄ _config.yml ‚îú‚îÄ‚îÄ Gemfile ‚îú‚îÄ‚îÄ index.markdown ‚îî‚îÄ‚îÄ _posts ‚îî‚îÄ‚îÄ 2023-01-22-welcome-to-jekyll.markdown Construir sitio web Con esto ya habremos creado nuestro blog, ahora queda construirlo para que Jekyll genere los ficheros necesarios para una vez levantemos el servidor, podamos ver la p√°gina como queremos, con sus im√°genes y su CSS. Para ello, tendremos que ejecutar el siguiente comando. docker run --rm --volume \"$PWD:/srv/jekyll\" -it jekyll/jekyll jekyll build Esto habr√° generado una carpeta llamada _site dentro de nuestro directorio, por lo que ya podremos levantar nuestro servidor para comprobar que funciona correctamente. Levantar servidor web Quiz√°s os d√© fallo la primera vez que arranque el servidor, para solucionarlo s√≥lo habr√° que agregar la siguiente l√≠nea en el fichero Gemfile: gem \"webrick\" As√≠ levantaremos el servidor web para ver la p√°gina. docker run --rm --name blog --volume \"$PWD:/srv/jekyll\" -p 4000:4000 -it jekyll/jekyll jekyll serve Para mostrarla, tendremos que abrir en el navegador la siguiente direcci√≥n, http://localhost:4000. Mientras el servidor est√© arrancado, cada vez que hagamos un cambio en los ficheros, con actualizar la p√°gina del navegador valdr√° para ver el cambio. Para acabar con el contenedor, valdr√° con pulsar ctrl-c para que termine el proceso. Cada vez que queremos levantar el servidor, con volver a ejecutar el anterior comando, lo tendremos listo. Con esto acaba el post, he utilizado Docker para trabajar con Jekyll porque prob√© en su d√≠a instalarlo directamente en el equipo y el problema de dependencias o al tener que cambiar de un equipo a otro es horrible, por lo que con estos nos ahorraremos muchos quebraderos de cabeza. Espero que os haya gustado y os haya servido de ayuda. ¬°Hasta la pr√≥xima!" }, { "title": "Habilitar logs en Rsyslog server", "url": "/posts/habilitar-logs-rsyslog-server/", "categories": "linux, rsyslog", "tags": "linux, rsyslog", "date": "2023-01-22 10:15:00 +0100", "content": "A veces rsyslog puede dar fallo, pero por defecto, estos logs no vienen habilitados, para ello tendremos que agregar estas l√≠neas en el fichero de configuraci√≥n rsyslog.conf $DebugFile &lt;nombre_fichero&gt; $DebugLevel &lt;0|1|2&gt; Donde &lt;nombre_fichero&gt; es donde queremos que se guarden los logs, podemos poner tanto el nombre de un fichero como la ruta completa con el nombre del fichero. Los niveles de $DebugLevel significan que 0 es desactivado, 1 habilitado el modo debug pero bajo demanda y 2 modo debug activado por completo. Este √∫ltimo tenemos que tener cuidado de no dejarlo activado si se reciben muchos logs, ya que llenar√° el disco duro en poco tiempo de la cantidad de informaci√≥n que escriben. Activar modo debug bajo demanda Si tenemos el nivel 1, para que se escriban logs en el fichero que hemos indicado antes, tenemos que hacer lo siguiente. Parar el servicio de rsyslog con systemctl stop rsyslog En otra terminal en el mismo servidor, ejecutar kill -USR1 $(cat /var/run/rsyslogd.pid) Espero que os haya gustado y os haya servido de ayuda. ¬°Hasta la pr√≥xima!" }, { "title": "Ampliar disco LVM", "url": "/posts/ampliar-disco-lvm/", "categories": "linux, lvm", "tags": "linux, lvm", "date": "2022-06-01 21:50:00 +0200", "content": "Para ampliar el espacio en un disco que ya est√© en el servidor, sin a√±adir un disco duro nuevo, hay que seguir los siguientes pasos. A√±adimos espacio en VWMare al disco. El resto de los pasos ya son dentro del servidor. Para no tener que reiniciar el servidor para que coja los cambios, ejecutamos el siguiente comando, cambiando la parte de sda por el disco que corresponda: echo 1 &gt; /sys/class/block/sda/device/rescan` Con fdisk -l veremos si se ha ampliado el disco. Ahora crearemos la partici√≥n. Utilizando el comando cfdisk /dev/&lt;disco&gt; nos aparecer√° un men√∫ para poder hacer la configuraci√≥n: En este caso lo haremos sobre el disco /dev/sda, que ten√≠a 24GB y le he aumentado en 1GB. Seleccionamos New, elegimos el tama√±o que queremos que tenga dicha partici√≥n y si va a ser primary o logical, yo recomiendo que si vamos a ampliar varias veces el disco, escojamos logical ya que la otra opci√≥n permite crear pocas. Seleccionamos Type y elegimos Linux LVM o escribimos el c√≥digo 8E. Guardamos los cambios con Write, aplicamos los cambios escribimos yes y salimos con Quit. Ejecutamos partprobe por si no ha refrescado los cambios. Ahora creamos el volumen f√≠sico para LVM con pvcreate /dev/&lt;partici√≥n&gt;, en este caso es /dev/sda4. A√±adimos el volumen f√≠sico al grupo, para ver los grupos de discos que existen ejecutamos vgs, con el comando vgextend &lt;vg&gt;/dev/sda4, donde &lt;vg&gt; es el grupo que nos haya mostrado el comando anterior. Ahora a√±adimos el espacio al volumen l√≥gico, para ver los vol√∫menes l√≥gicos que hay ejecutamos lvs y para saber el nombre completo tambi√©n podemos verlo con df. El formato es algo tal que as√≠ /dev/mapper/&lt;vg&gt;-&lt;lv&gt;, &lt;lv&gt; es el resultado del comando lvs. Un ejemplo del comando en cuesti√≥n ser√≠a lvextend -r -l +100%FREE /dev/mapper/pvg0-lv--root Es importante tener el -r para que aplique los cambios autom√°ticamente. Si queremos seleccionar un porcentaje como en el ejemplo, hemos dicho que coja todo el espacio disponible, es con la opci√≥n -l (es una L min√∫scula), en cambio si queremos que sea una cantidad especifica de GB, la opci√≥n ser√° -L &lt;n¬∫ de GB&gt;GB. Espero que os haya gustado y os haya servido de ayuda. ¬°Hasta la pr√≥xima!" }, { "title": "Configurar Rsyslog para recolectar logs de Docker", "url": "/posts/configurar-rsyslog-logs-docker/", "categories": "linux, docker, rsyslog", "tags": "linux, docker, rsyslog", "date": "2022-03-07 18:00:00 +0100", "content": "En el post anterior, configuramos un servidor rsyslog para centralizar todos los logs de nuestra infraestructura. En este, configuraremos tanto el servidor Docker que desplegamos para que env√≠e los logs al rsyslog, como el servidor rsyslog para que los recoja. Configurar el servidor de rsyslog Primero vamos a crear un fichero de configuraci√≥n que recolecte todos los logs del demonio de Docker. Para ello crearemos el fichero /etc/rsyslog.d/docker-daemon.log en el servidor rsyslog con el siguiente contenido: $template DockerLogs, \"/var/log/docker/daemon.log\" if $programname startswith 'dockerd' then -?DockerLogs &amp; stop Con esto haremos que todo log que empiece por dockerd se guarde en la ruta definida. Ahora, crearemos el fichero de configuraci√≥n para los logs de los contenedores, esto tambi√©n se har√° en el servidor rsyslog. El nombre del fichero ser√° /etc/rsyslog.d/docker-containers.log y tendr√° lo siguiente: $template DockerContainerLogs,\"/var/log/docker/%hostname%_%syslogtag:R,ERE,1,ZERO:.*container_name/([^\\[]+)--end%.log\" if $syslogtag contains 'container_name' then -?DockerContainerLogs &amp; stop Aqu√≠ indicamos que guarde en la ruta definida e indique el nombre del host que ha enviado dichos logs. Reiniciamos el servicio para que se apliquen las configuraciones que acabamos de agregar: systemctl restart rsyslog Configurar Docker para que env√≠e los logs a rsyslog Una vez configurado rsyslog, vamos a configurar el fichero del demonio de Docker, daemon.json, que se encuentra en /etc/docker para servidores Linux y %userprofile%\\.docker en Windows con Docker Desktop. Tendremos que a√±adir el siguiente texto a continuaci√≥n de lo que ya haya en el fichero (he tenido que colocar una imagen porque no reconoce .Name como texto por el formato y lo borra): Esto etiquetar√° a todos los contenedores con container_name y el nombre de dicho contenedor para que rsyslog sea capaz de procesarlo seg√∫n lo hemos indicado anteriormente. Reiniciamos el servicio de Docker para aplicar cambios: sudo systemctl restart docker Con esto ya empezar√°n a llegar los logs de Docker y de los contenedores que ejecute, teniendo de esta forma una manera m√°s c√≥moda de poder visualizar que puede ocurrir con un contenedor y no tener que acceder a todos los servidores Docker que tengamos en busca de √©l. Espero que os haya gustado y os haya servido de ayuda. ¬°Hasta la pr√≥xima!" }, { "title": "Recolectar cualquier logs con Rsyslog", "url": "/posts/configurar-rsyslog-personalizados/", "categories": "linux, rsyslog", "tags": "linux, rsyslog", "date": "2021-03-15 19:35:00 +0100", "content": "En esta ocasi√≥n, vamos a configurar Rsyslog para que recolecte cualquier log que le indiquemos. Esto es muy √∫til si tenemos un aplicaci√≥n propia que genere logs o cualquier otra aplicaci√≥n como puede ser Tomcat, por ejemplo, adem√°s de ser f√°cil de implementar (una vez te has peleado con ello). Fichero configuraci√≥n Vamos a crear el fichero de configuraci√≥n necesario en el cliente para que mande los logs al servidor central. El servidor central debe estar configurado como indicamos en este post. Una vez hecho eso, vamos al grano. Antes de crear el fichero, tenemos que crear el directorio de trabajo para rsyslog, en este caso /var/spool/rsyslog. El comando para ello es: mkdir /var/spool/rsyslog Creamos un fichero llamado &lt;aplicaci√≥n&gt;.conf, pod√©is ponerle el nombre que quer√°is, dentro de /etc/rsyslog.d/ con el siguiente contenido: $ModLoad imfile $InputFilePollInterval 10 $WorkDirectory /var/spool/rsyslog $template Rfc5424Format,\"&lt;%PRI%&gt;1 %TIMESTAMP:::date-rfc3339% %HOSTNAME% %APP-NAME% %PROCID% %MSGID% %STRUCTURED-DATA% %msg%\" $InputFileName &lt;/ruta/fichero&gt; $InputFileTag &lt;aplicaci√≥n&gt;-log $InputFileStateFile stat-&lt;aplicaci√≥n&gt;-log $InputFileSeverity info $InputFilePersistStateInterval 20000 $InputRunFileMonitor if $programname == '&lt;aplicaci√≥n&gt;-log' then @@&lt;ip_servidor&gt;:514;Rfc5424Format if $programname == '&lt;aplicaci√≥n&gt;-log' then stop A primera vista esto asusta un poco, pero vamos a ir desglosando que funci√≥n tiene cada l√≠nea del fichero. Los siguientes par√°metros son globales. $ModLoad imfile carga el m√≥dulo de rsyslog. Lee l√≠nea a l√≠nea el fichero que le pasemos como par√°metro. $InputFilePollInterval especifica la frecuencia con la que comprueba los archivos para obtener nuevos datos. $WorkDirectory el directorio de trabajo de rsyslog. Aqu√≠ se guardan los llamados state file, que contienen temporalmente los nuevos datos que se recogen para ser enviados. $template el formato en el que se van a mandar los logs al servidor. Esta parte es muy importante, porque probando distintos formatos, me ha llegado a crear un fichero por cada l√≠nea que se recog√≠a en el fichero. Estos par√°metros son espec√≠ficos para el archivo con que vamos a trabajar. $InputFileName el fichero que queremos que recolecte la informaci√≥n y env√≠e. $InputFileTag el nombre del fichero que se crear√° en el servidor y donde se mandar√° la informaci√≥n. $InputFileStateFile el nombre de los state files mencionados antes. $InputFileSeverity la severidad que se le asigna a las l√≠neas que lee. $InputFilePersistStateInterval especifica la frecuencia con la que se escribir√° el archivo de estado al procesar el archivo de entrada. Esta configuraci√≥n se puede utilizar para evitar duplicidad de mensajes por errores como un corte de energ√≠a. $InputRunFileMonitor activa que se monitorice el fichero. Las dos √∫ltimas 2 l√≠neas indican que si les llega una petici√≥n de tratar el fichero con el tag indicado, lo manden a un servidor por TCP (2 @ indica TCP, 1 @ por UDP) aplicando el formato que hemos definido. IMPORTANTE. Si est√°is trabajando con CentOS y ten√©is el SELinux activado, ten√©is que cambiar el tipo del fichero por var_log_t con el comando: chcon -t var_log_t &lt;fichero&gt; Ten√©is que tener en cuenta que si la carpeta se encuentra fuera de /var/log tambi√©n ten√©is que cambiarle el tipo a dicha carpeta, porque si no rsyslog no podr√° acceder a ella. Puedes jugar bastante con Rsyslog y todas las posibilidades que te ofrece, facilitando mucho el trabajo cuando tienes que revisar muchos logs de distintos servidores. Espero que os haya gustado y os haya servido de ayuda. ¬°Hasta la pr√≥xima!" }, { "title": "Configurar servidor Rsyslog en Linux", "url": "/posts/configurar-servidor-rsyslog-linux/", "categories": "linux, rsyslog", "tags": "linux, rsyslog", "date": "2021-02-22 10:15:00 +0100", "content": "Los logs, esos ficheros que recogen lo que sucede en nuestro sistema y sus aplicaciones, es una parte muy importante en el d√≠a a d√≠a de nuestro trabajo como sysadmin, pero tener que revisar muchos de estos puede llegar a ser una locura, m√°s ahora que el n√∫mero de servidores que tenemos que tener bajo control es cada vez m√°s y m√°s grande. Algo m√°s que recomendable es tener un servidor central que se encargue de recoger estos logs y poder acceder a ellos de forma m√°s c√≥moda, que no tener que estar revisando uno a uno los servidores encontrando el dichoso fichero. Por eso hoy os contar√© la forma de configurar una forma centralizada de recogida de logs y que en caso de ca√≠da del servidor rsyslog, no perdamos ning√∫n fichero guard√°ndolos en el cliente en una cola que los mantenga hasta que sea necesario y enviarlos de nuevo. Instalar y configurar rsyslog server Rsyslog es un paquete preinstalado en los sistemas Linux, pero en caso de no tenerlo instalado, podemos obtenerlo de la siguiente manera yum update -y; yum install rsyslog -y # CentOS 7 apt update -y; apt install rsyslog -y # Ubuntu Editamos el fichero /etc/rsyslog.conf para configurar que la m√°quina act√∫e como servidor. Descomentamos las siguientes l√≠neas para activar el protocolo UDP y el puerto 514, este √∫ltimo se puede cambiar por otro de nuestra elecci√≥n. $ModLoad imudp $UDPServerRun 514 Para las conexiones TCP, debemos descomentar las siguientes l√≠neas, e igual que el anterior, podemos poner un puerto a nuestra elecci√≥n. $ModLoad imudp InputTCPServerRun 514 Configuramos una regla para que recoja todos los logs y los guarde en una carpeta con el nombre del host que se lo env√≠a. Para tener una mejor organizaci√≥n y no trabajar sobre el fichero principal, crearemos un fichero remotelogs.conf, aunque se puede llamar como queramos siempre que acabe en .conf dentro de la carpeta /etc/rsyslog.d/ template(name=\"TmplLogRemoto\" type=\"list\") { constant(value=\"/var/log/rsyslog/\") property(name=\"HOSTNAME\") constant(value=\"/\") property(name=\"programname\" SecurePath=\"replace\") constant(value=\".log\") } *.* ?TmplLogRemoto Lo que hemos hecho es definir que los logs se guarden en /var/log/rsyslog, despu√©s tiene que crear una carpeta con el nombre del host por cada cliente que a√±adamos posteriormente y luego guarde los registros en ella. Reiniciamos el servicio. systemctl restart rsyslog Comprobamos que el servidor est√° a la escucha en los puertos indicados anteriormente. ss -tulnp | grep \"rsyslog\" Ahora tenemos que permitir las conexiones a esos puertos en firewalld. firewall-cmd --permanent --add-port=514/udp firewall-cmd --permanent --add-port=514/tcp firewall-cmd --reload En CentOS, en caso de tener habilitado SELinux, hay que permitir el tr√°fico de esos puertos, por lo que ejecutaremos los siguientes comandos: semanage port -a -t syslogd_port_t -p udp 514 semanage port -a -t syslogd_port_t -p tcp 514 Si nos aparece que el comando semanage no existe, tenemos que ejecutar el siguiente comando para poder instalarlo. yum install policycoreutils-python -y Configurar rsyslog en el cliente En caso de no tenerlo instalado, hay que seguir los pasos indicados en la parte del servidor, es el mismo paquete. Como en el caso anterior, para tener una mejor organizaci√≥n, crearemos un fichero en /etc/rsyslog.d llamado como queramos con la extensi√≥n .conf, en mi caso clientlogs.conf, y a√±adimos las siguientes l√≠neas, sustituyendo por la IP del servidor que hemos configurado en el apartado Target. action(type=\"omfwd\" queue.type=\"LinkedList\" queue.filename=\"queue_logs\" action.resumeRetryCount=\"-1\" queue.saveonshutdown=\"on\" Target=\"&lt;ip_servidor&gt;\" Port=\"514\" Protocol=\"tcp\") Donde: queue.type habilita que la cola de logs sea del tipo LinkedList, que asigna la memoria s√≥lo cuando es necesaria. queue.filename define el nombre del disco que almacenar√° los logs hasta que se env√≠an al servidor. Se guarda en /var/lib/rsyslog. action.resumeRetryCount=\"-1\" evita que rsyslog descarte los mensajes al volver a intentar conectarse si el servidor no responde. queue.saveonshutdown guarda datos en memoria si rsyslog se apaga. La √∫ltima l√≠nea reenv√≠a todos los mensajes recibidos al servidor por TCP. El puerto es opcional. Para mandar un custom log y realizar una prueba, si ejecutamos logger &lt;texto&gt;, mandar√° un log al servidor y comprobaremos si la comunicaci√≥n se realiza correctamente. De esta manera, ya tendremos los logs de todos nuestros servidores en un √∫nico sitio y no tendremos que estar conect√°ndonos en mil sitios distintos. En posteriores posts tengo pensado explicar c√≥mo podemos hacer que la comunicaci√≥n entre servidor/cliente se haga bajo un certificado que cifre la informaci√≥n que se env√≠a, crear templates en los clientes para aplicaciones como Docker, Tomcat y dem√°s, adem√°s de desplegar Logstash, Elasticsearch o Kibana para poder trabajar con los logs de forma m√°s visual. Espero que os haya gustado y os haya servido de ayuda. ¬°Hasta la pr√≥xima!" }, { "title": "Configurar IP est√°tica en CentOS", "url": "/posts/configurar-ip-estatica-centos/", "categories": "linux, centos, raspberry pi", "tags": "linux, centos, raspberry pi", "date": "2021-02-22 10:15:00 +0100", "content": "En el post anterior instalamos CentOS en nuestra Raspberry, hoy la configuraremos para poder darle uso. Normalmente los equipos obtienen su direcci√≥n IP a trav√©s del protocolo DHCP (Dynamic Network Configuration Protocol), un servicio que proporciona una IP din√°mica, adem√°s de la m√°scara de subred, la puerta de enlace y los DNS a los equipos de la red. En redes dom√©sticas quien cumple esta funci√≥n suele ser el router que nos proporciona nuestro proveedor de internet. Pero en entornos profesionales, o como es en nuestro caso, no queremos que nuestro servidor tenga un IP que le haya proporcionado el DHCP y que pueda cambiar. Para ello se configura un IP est√°tica o manual y tener bajo control la IP de ese servidor. Conectarnos al servidor Podemos acceder al servidor conectando una pantalla y un teclado directamente a la Raspberry Pi y trabajar directamente ah√≠ o podemos conectarnos por SSH. Para poder conectarnos por SSH, tendremos que averiguar su IP. Podemos saber esto yendo al DHCP del router o hacer un escaneo de red con herramientas como Advanced IP Scanner. Una vez averiguada la IP, podemos conectarnos por SSH utilizando Putty o MobaXterm, eso os lo dejo al gusto de cada uno. Ponemos la IP del servidor y le ponemos las credenciales, que ser√° el usuario root y la contrase√±a que hayamos definido. Configurar la IP est√°tica Est√° va a ser la configuraci√≥n que yo voy a hacer en mi servidor. Direcci√≥n IP: 192.168.0.150 Subred: 255.255.255.0 Puerta de enlace (Router): 192.168.0.1 Servidor DNS 1: 192.168.0.1 Servidor DNS 2: 1.1.1.1 Averiguar interfaces de red en nuestro servidor Puedes saber las interfaces que tiene el servidor ejecutando el comando: ifconfig -a O ip a El resultado del comando ser√° parecido a lo que muestro a continuaci√≥n. En mi caso quiero cambiar la IP a la interfaz eth0, que es la interfaz cableada. lo es la interfaz de loopback, donde est√° configurado la IP 127.0.0.1 o localhost. wlan0 es la interfaz de conexi√≥n inal√°mbrica, para ir por Wi-FI. 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000 link/ether dc:a6:32:a3:a8:b4 brd ff:ff:ff:ff:ff:ff inet 192.168.0.24/24 brd 192.168.0.255 scope global noprefixroute dynamic eth0 valid_lft 2733sec preferred_lft 2733sec inet6 fe80::806d:e80d:c6fb:69d6/64 scope link noprefixroute valid_lft forever preferred_lft forever 3: wlan0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc pfifo_fast state DOWN group default qlen 1000 link/ether 62:c3:19:db:2a:2c brd ff:ff:ff:ff:ff:ff Configuraci√≥n IP est√°tica M√©todo 1 Para este m√©todo, editaremos el fichero de la interfaz de red que se encuentra en el directorio /etc/sysconfig/network-scripts. Para la interfaz eth0, crearemos el fichero ifcfg-eth0. vi /etc/sysconfig/network-scripts/ifcfg-eth0 El contenido que tenemos que poner en el fichero. # MAC de la interfaz # HWADDR=dc:a6:32:a3:a8:b4 TYPE=Ethernet BOOTPROTO=none # IP est√°tica # IPADDR=192.168.0.150 # Subred # NETMASK=255.255.255.0 # Puerta de enlace # GATEWAY=192.168.0.1 # Servidores DNS # DNS1=192.168.0.1 DNS2=1.1.1.1 DEFROUTE=yes IPV4_FAILURE_FATAL=no # Deshabilitar ipv6 # IPV6INIT=no # Nombre de la interfaz # NAME=eth0 DEVICE=eth0 # Activar la interfaz en el arranque # ONBOOT=yes M√©todo 2 Tambi√©n podemos usar nmtui, una interfaz de usuario en texto para configurar interfaces de red. En caso de no tenerlo instalado, podemos obtenerlo con el comando yum install NetworkManager-tui nmtui Seleccionamos Edit a connection y presionamos Enter. Elegimos la interfaz de red y Edit Configuramos la IP y le damos a OK Reiniciamos la red Finalmente, reiniciamos el servicio de red para que se hagan efectivo los cambios que hemos hecho. Cuando hagamos esto no echar√° de la sesi√≥n porque se ha cambiado de IP. systemctl restart network Volvemos a ejecutar el comando ip a para comprobar que el cambio se ha hecho efectivo. Tip final. Cambiar hostname Ya que hemos configurado la IP del servidor, vamos a cambiar tambi√©n el nombre del mismo por alguno m√°s descriptivo y porque personalmente no me gusta que los servidores se llamen localhost. Aprovechando que hemos trabajado con nmtui, haremos el cambio desde ah√≠. Seleccionamos Set system hostname y presionamos Enter. Ah√≠ ponemos el nombre que queramos que tenga el servidor y presionamos OK. La gente que quiera hacerlo por comando, tiene a su disposici√≥n hostnamectl. Para cambiar el nombre ser√≠a de la siguiente manera: hostnamectl set-hostname \"&lt;nombre_servidor&gt;\" Para que se efect√∫en los cambios podemos bien reiniciar el servidor o el servicio systemctl restart systemd-hostnamed. Para comprobar que el cambio se ha hecho con hostname podremos confirmar que es as√≠. Para el nombre que aparece en la terminal [root@localhost ~]#, con salir de la sesi√≥n SSH y volver a conectarnos ya nos aparecer√°. Espero que os haya gustado y os haya servido de ayuda. ¬°Hasta la pr√≥xima!" }, { "title": "Instalar CentOS en Raspberry Pi", "url": "/posts/instalar-centos-raspberry-pi/", "categories": "linux, centos, raspberry pi", "tags": "linux, centos, raspberry pi", "date": "2021-02-21 20:48:00 +0100", "content": "Desde hace tiempo tengo varias Raspberry Pi muertas de risa y quiero darles un alg√∫n uso, pero no quiero utilizar Raspberry Pi OS, as√≠ que instalaremos CentOS 7 para arquitecturas ARM. Como curiosidad, Raspberry Pi OS, antes Raspbian, es el sistema operativo que se suele utilizar en Raspberry Pi basado en Debian. En caso utilizar√© una Raspberry Pi 4 de 4GB de RAM (Enlace art√≠culo Amazon, post no esponsorizado por Amazon). Descargar CentOS para Raspberry Pi Primero tendremos que descargarnos la imagen ISO de los repositorios oficiales de CentOS. En esta p√°gina tendremos un listado de los mirrors de CentOS, as√≠ que elegiremos uno de la lista. Una vez en el listado tendremos que elegir el fichero que contiene el n√∫mero 4 en el nombre, indicando que es para la Raspberry Pi 4. Un ejemplo ser√≠a CentOS-Userland-7-armv7hl-RaspberryPI-Minimal-4-2009-sda.raw.xz, que contiene Minimal-4. Esta es la opci√≥n que no incluye una interfaz gr√°fica, s√≥lo terminal. El resto son versiones para la Raspberry Pi 3, tanto la versi√≥n minimal como con distintos entornos de escritorio. Yo elegir√© la versi√≥n Minimal, porque no me interesa tener un entorno gr√°fico, ya que me conectar√© a trav√©s de SSH a ella. Una vez descargada, comprobamos si el hash del fichero descargado corresponde con el publicado en la p√°gina. Normalmente es el fichero que se encuentra en el mismo directorio de donde hemos sacado la ISO llamado sha256sum.txt. En √©l saldr√°n los hashes en SHA256 de cada uno de los ficheros. Para comprobarlo lo haremos de las siguientes maneras dependiendo del sistema operativo. Windows. Desde una consola de powershell, iremos al directorio donde hemos descargado el fichero y ejecutaremos el siguiente comando: Get-FileHash &lt;fichero&gt; Linux. Desde la terminal, iremos al directorio donde hemos descargado el fichero y ejecutaremos el siguiente comando: sha256sum &lt;fichero&gt; En este caso usaremos este comando porque queremos obtener el hash SHA256. Obtenido el hash con el comando, ahora s√≥lo es comparar con el que se muestra en la p√°gina. No me se√°is cafres y con que comprob√©is los primeros 6 d√≠gitos y los 6 √∫ltimos valdr√≠a. Montar CentOS en la tarjeta SD Cuando hayamos descargado la imagen de CentOS y comprobado que el hash es correcto, vamos a montar la ISO en nuestra tarjeta SD. Yo en este caso utilizar√© Rufus, pero pod√©is usar otros como BalenaEtcher. A tener en cuenta: los datos que tengamos en la tarjeta SD se borrar√°n porque √©sta tiene que formatearse. En caso de que vuestro equipo no tenga lector de tarjetas SD, como me pasa a m√≠, pod√©is comprar un lector SD/Micro SD adaptador USB. Una vez terminado, pondremos la tarjeta en nuestra Raspberry Pi y la conectaremos a la corriente para encenderlo. Trabajando con CentOS Para poder conectarnos a nuestra Raspberry Pi directamente, tenemos que tener un adaptador Micro HDMI a HDMI, yo os recomiendo comprar este pack que viene con una carcasa para darle un mejor aspecto. (Enlace art√≠culo Amazon) Adem√°s de un teclado USB para poder escribir en la terminal. El usuario por defecto es root. La contrase√±a por defecto es centos. Es recomendable cambiar la contrase√±a, para ello ejecutamos el comando: passwd Expandimos la partici√≥n de root para que coja el tama√±o m√°ximo de la tarjeta SD con rootfs-expand. Configuramos la zona horaria que corresponda con timedatectl set-timezone Europe/Madrid. Ahora que hemos hecho esos cambios, vamos a actualizarla e instalar alg√∫n paquete. Para manejar los paquetes en CentOS, usamos el comando yum. Actualizar: yum update Instalar: yum install &lt;nombre_paquete&gt; Eliminar: yum remove &lt;nombre_paquete&gt; Por ejemplo, podemos instalar el editor de texto vim y despu√©s eliminarlo de la siguiente manera. yum install vim yum remove vim Si al final de estos comandos ponemos la opci√≥n -y, no ser√° necesario confirmar que queremos actualizar, instalar o eliminar un paquete. Con esto podemos dar por finalizado este post, en posteriores posts le daremos uso a la Raspberry y realizarle una configuraci√≥n m√°s avanzada. Aporte extra Trabajando con CentOS en Raspberry Pi por defecto no vienen disponibles los repositorios epel, pero podemos activarlos sin mayor inconveniente. En la Wiki de CentOS te indican c√≥mo activarlo. Para ello hay que ejecutar lo siguiente: cat &gt; /etc/yum.repos.d/epel.repo &lt;&lt; EOF [epel] name=Epel rebuild for armhfp baseurl=https://armv7.dev.centos.org/repodir/epel-pass-1/ enabled=1 gpgcheck=0 EOF Eso crear√° un fichero indicando que tambi√©n busque en los repositorios de la URL indicada. Una vez hecho, actualizamos con yum update -y y ya podremos seguir trabajando. Espero que os haya gustado y os haya servido de ayuda. ¬°Hasta la pr√≥xima!" }, { "title": "Instalar Docker en Windows", "url": "/posts/instalar-docker-windows/", "categories": "windows, docker", "tags": "windows, docker", "date": "2021-02-19 14:55:00 +0100", "content": "En el post anterior, aprendimos c√≥mo instalar Docker en un servidor CentOS. En esta ocasi√≥n haremos lo propio en un Windows 10, para poder trabajar con la herramienta en nuestro equipo. Requisitos Windows 10 64-bit: Pro, Enterprise o Education. 4GB de RAM. Tener habilitado la virtualizaci√≥n en la BIOS. En la web oficial de Docker te indican que necesitas tener habilitado el cliente de HyperV, pero como fue en mi caso, al tener habilitado esta caracter√≠stica, aplicaciones como VMware Workstation dejan de funcionar. Por suerte, a partir de Windows 10 2004, sali√≥ Windows Subsystem for Linux (WSL) 2, permitiendo correr contenedores Docker de forma nativa, adem√°s de reducir el consumo de CPU y memoria de forma considerable. Para poder seguir este tutorial, tenemos que tener actualizado Windows 10 a la versi√≥n 2004 o superior. Instalaci√≥n WSL 2 Para poder tener instalado WSL 2, debemos instalar antes la versi√≥n 1. Desde una consola de Powershell con permisos de administrador, ejecutamos el siguiente comando: dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart Adem√°s tenemos que instalar la caracter√≠stica ‚ÄúVirtual Machine Platform‚Äù o su nombre en espa√±ol ‚ÄúPlataforma de m√°quina virtual‚Äù. dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart Reiniciamos el equipo y seguimos con el siguiente paso. En caso de tener ya instalado WSL 1, estos pasos pueden que nos den error. Para actualizar a la versi√≥n 2, tenemos que descargar el siguiente paquete de la web de Windows. Una vez descargado, ejecutamos el fichero y seguimos los pasos indicados. WSL 2 por defecto Por defecto nuestro sistema utilizar√° WSL 1, por lo que tenemos que cambiar dicha configuraci√≥n para que utilice la versi√≥n 2. Ejecutamos el siguiente comando en la consola de Powershell. wsl --set-default-version 2 Para comprobar las distribuciones que tenemos instaladas, podemos hacerlo con el siguiente comando. wsl -l -v Instalar y configurar Docker Ya es hora de instalar Docker despu√©s de haber seguido los pasos anteriores. Descargamos Docker Desktop for Windows desde la web de Docker o de Docker Hub y lo instalamos. En la instalaci√≥n podremos habilitar la compatibilidad con WSL 2 y una vez termine, tendremos que reiniciar el equipo. Ahora que el equipo haya arrancado e iniciemos Docker Desktop, nos aparecer√° una ventana en la aplicaci√≥n con un tutorial que podemos hacer u omitir. Para comprobar que todo ha ido bien, abrimos una consola de Powershell y ejecutamos la imagen hello-world como la anterior vez. docker run hello-world Es una herramienta muy potente que yo utilizo para realizar desarrollos y pruebas sin tener que desplegar una m√°quina virtual, ahorrando mucho tiempo en el proceso, por lo cual la recomiendo. Espero que os haya gustado y os haya servido de ayuda. ¬°Hasta la pr√≥xima!" }, { "title": "Instalar Docker en Ubuntu", "url": "/posts/instalar-docker-ubuntu/", "categories": "linux, docker, ubuntu", "tags": "linux, docker, ubuntu", "date": "2021-02-17 15:45:00 +0100", "content": "En el post anterior aprendimos a instalar Docker, en este post haremos lo propio Ubuntu. Aunque haya varias maneras de instalarlo, en esta caso utilizaremos la instalaci√≥n por repositorio. Paso 1: Eliminar versiones anteriores de Docker Siempre viene bien hacer una limpieza por si el equipo con el que estamos ya tiene una versi√≥n anterior de Docker instalada. sudo apt remove docker docker-engine docker.io containerd runc Paso 2: Instalamos dependencias El siguiente paso es instalar las dependencias que necesita Docker para instalarse. sudo apt update sudo apt install \\ ca-certificates \\ curl \\ gnupg \\ lsb-release Paso 3: Configurar el repositorio de Docker Agregamos la key GPG oficial de Docker. sudo mkdir -p /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg Con este comando a√±adimos el repositorio dentro de la carpeta sources.list.d echo \\ \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null Y actualizamos los repositorios. sudo apt update Paso 4: Instalamos Docker CE (Community Edition) Finalmente, instalamos Docker ejecutando el siguiente comando: sudo apt-get install docker-ce docker-ce-cli containerd.io docker-compose-plugin Paso 5: Administrar el servicio de Docker Aunque hayamos instalado Docker, todav√≠a no se est√° ejecutando. Tenemos que configurarlo ejecutando los siguientes comandos: Habilitamos que el servicio inicie con el arranque del servidor: systemctl enable docker Iniciamos el servicio de Docker para que empiece a funcionar: systemctl start docker Podemos habilitar el servicio y que arranque a la vez si ejecutamos el siguiente comando: systemctl enable docker --now Comprobamos que el servicio est√° iniciado: systemctl status docker Nos deber√≠a aparecer una l√≠nea verde que indica que el servicio est√° en funcionamiento. Paso 6: Trabajando con Docker Tendremos que agregar el usuario con el que hemos instalado Docker al grupo correspondiente para que pueda ejecutar los comandos sin tener que utilizar sudo constantemente. sudo usermod -aG docker $USER Una vez hecho esto, vamos a comprobar que Docker funciona correctamente. docker run hello-world Con este comando, descargaremos el contenedor hello-world y lo ejecutaremos una vez haya terminado. Si todo ha funcionado correctamente, nos aparecer√° el siguiente mensaje: Hello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/ Espero que os haya gustado y os haya servido de ayuda. ¬°Hasta la pr√≥xima!" }, { "title": "Instalar Docker en CentOS", "url": "/posts/instalar-docker-centos/", "categories": "linux, docker, centos", "tags": "linux, docker, centos", "date": "2021-02-16 15:45:00 +0100", "content": "Hoy hablaremos de una herramienta muy interesante y que cada vez tiene m√°s cabida en nuestro trabajo como sysadmin, Docker. Aprenderemos a instalar Docker en Linux. Vamos a empezar hablando de lo que es Docker, para tener una idea. Docker es una tecnolog√≠a de creaci√≥n de contenedores que permite la creaci√≥n y el uso de contenedores Linux o Windows. Suelta la explicaci√≥n que viene en la documentaci√≥n, seguimos desglosando qu√© es Docker. Docker revoluciona el concepto que tenemos de m√°quinas virtuales, ya no virtualiza un sistema operativo como venimos haciendo desde hace a√±os, lo que hace es virtualizar el software encapsul√°ndolo, gracias a que utiliza caracter√≠sticas del kernel de Linux, de ah√≠ que se llamen contenedores, haciendo referencia a las cajas de los barcos. Dentro del contenedor se ejecutan todas aquellas cosas que la aplicaci√≥n necesita para funcionar y la propia aplicaci√≥n. El contenedor es la aplicaci√≥n en funcionamiento por as√≠ decirlo. En posts posteriores seguiremos viendo m√°s caracter√≠sticas del software y explicando c√≥mo se compone, por ahora vamos al l√≠o, que a eso hemos venido. Paso 1: Actualizar paquetes En la terminal, escribiremos el siguiente comando para actualizar los repositorios de nuestro servidor: sudo yum update -y Paso 2: Instalamos dependencias El siguiente paso es instalar las dependencias que necesita Docker para instalarse. sudo yum install yum-utils device-mapper-persistent-data lvm2 -y Paso 3: Agregamos el repositorio de Docker a CentOS Por defecto, Docker no viene en los repositorios oficiales de Centos, por lo que tenemos que agregarlos nosotros para poder instalarlo. Instalaremos la versi√≥n estable. sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo Paso 4: Instalamos Docker CE (Community Edition) Finalmente, instalamos Docker ejecutando el siguiente comando: sudo yum install docker-ce Paso 5: Administrar el servicio de Docker Aunque hayamos instalado Docker, todav√≠a no se est√° ejecutando. Tenemos que configurarlo ejecutando los siguientes comandos: Habilitamos que el servicio inicie con el arranque del servidor: systemctl enable docker Iniciamos el servicio de Docker para que empiece a funcionar: systemctl start docker Comprobamos que el servicio est√° iniciado: systemctl status docker Nos deber√≠a aparecer una l√≠nea verde que indica que el servicio est√° en funcionamiento. Paso 6: Trabajando con Docker En entornos de laboratorio se suele utilizar el usuario root para poder trabajar c√≥modamente, pero una buena pr√°ctica es darle permisos a un usuario que no sea root sobre el grupo docker. Si est√°is usando el usuario al que hab√©is dado permisos, ten√©is que reiniciar la sesi√≥n. sudo usermod -aG docker $USER Una vez hecho esto, vamos a comprobar que Docker funciona correctamente. docker run hello-world Con este comando, descargaremos el contenedor hello-world y lo ejecutaremos una vez haya terminado. Si todo ha funcionado correctamente, nos aparecer√° el siguiente mensaje: Hello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/ En siguientes posts explicaremos m√°s apartados de Docker, como qu√© son las im√°genes, c√≥mo crearlos con Dockerfile, descargarnos im√°genes sin ejecutarlas o acceder a los mismos con una consola interactiva. Espero que os haya gustado y os haya servido de ayuda. ¬°Hasta la pr√≥xima!" }, { "title": "Mi primerito post", "url": "/posts/mi-primerito-blog/", "categories": "Blog", "tags": "blog", "date": "2021-02-14 22:48:00 +0100", "content": "Este es el primer post, espero que el primero de muchos, donde cuento la idea que tengo sobre el blog. Desde hace tiempo he querido tener una p√°gina web propia y subir en ella todas las cosas que s√©, ya sea por curiosidad o porque me he tenido que pelear con ellas en el trabajo. La verdad es que no tengo muy claro c√≥mo llevar√© esto, la periodicidad de los posts o siquiera si alguien, que no sea un conocido al que le pase el enlace ense√±√°ndole lo nuevo que vaya subiendo, llegar√° a descubrir por casualidad este peque√±o blog. Me har√≠a mucha ilusi√≥n que hubiera una persona a la que le resultase de utilidad lo que vaya a escribir, siempre gusta saber que has ense√±ado algo nuevo a alguien. Con estas l√≠neas expreso brevemente lo que me ronda por la cabeza respecto a esto. Espero poder escribir muchos posts y que esto vaya viento en popa. Un saludo. ¬°Hasta la pr√≥xima!" } ]
